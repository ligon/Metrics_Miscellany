* Utils
** Unary Matrix Operators
#+begin_src python :tangle metrics_miscellany/utils.py
import numpy as np
from scipy import sparse as scipy_sparse
import pandas as pd

def inv(A):
    """Inverse of square pandas DataFrame."""
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.inv(A)
    return pd.DataFrame(B,columns=A.columns,index=A.index)

def pinv(A):
    """Moore-Penrose pseudo-inverse of A.
    """
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.pinv(A)
    return pd.DataFrame(B,columns=A.index,index=A.columns)


def diag(X,sparse=True):

    try:
        assert X.shape[0] == X.shape[1]
        d = pd.Series(np.diag(X),index=X.index)
    except IndexError: # X is a series?
        if sparse:
            # We can wind up blowing ram if not careful...
            d = scipy_sparse.diags(X.values)
            d = pd.DataFrame.sparse.from_spmatrix(d,index=X.index,columns=X.index)
        else:
            raise NotImplementedError
    except AttributeError: # Not a pandas object?
        d = np.diag(X)

    return d

def add(X,Y,column_name='X'):
    '''
    This function allows to do the additive operation between different and keep the columns
    Parameters
    ----------
    X,Y: data
    column_name: The columns name that we would maintain, default as the column name of X

    Return
    -----
    X+Y
    '''
    assert Y.shape == X.shape,'Input has different shape'
    if column_name == 'X':
        return pd.DataFrame(np.array(X)+np.array(Y),columns=X.columns)
    elif column_name == 'Y':
        return pd.DataFrame(np.array(X)+np.array(Y),columns=Y.columns)
    else:
        return pd.DataFrame(np.array(X)+np.array(Y),columns=column_name)

def sub(X,Y,column_name='X'):
    '''
    This function allows to do the subtraction operation between different and keep the columns
    Parameters
    ----------
    X,Y: data
    Column_name: The columns name that we would maintain, default as the column name of X

    Return
    -----
    X-Y
    '''
    assert Y.shape == X.shape,'Input has different shape'
    if column_name == 'X':
        return pd.DataFrame(np.array(X)-np.array(Y),columns=X.columns)
    elif column_name == 'Y':
        return pd.DataFrame(np.array(X)-np.array(Y),columns=Y.columns)
    else:
        return pd.DataFrame(np.array(X)-np.array(Y),columns=column_name)

def dot(X,Y,column_name='X'):
    '''
    This function allows to do the dot product(hadamard product) operation between different and keep the columns
    Parameters
    ----------
    X,Y: data
    Column_name: The columns name that we would maintain, default as the column name of X

    Return
    -----
    X*Y
    '''
    assert Y.shape[0] == X.shape[0],"The rows don't match"
    result = pd.DataFrame(np.array(X)*np.array(Y))
    assert result.shape[1] == len(column_name),"The columns name don't match the number of columns"
    result.columns = column_name
    return result
    

#+end_src
** Binary Matrix Products
#+begin_src python :tangle metrics_miscellany/utils.py
def outer(S,T):
    """Outer product of two series (vectors) S & T.
    """
    return pd.DataFrame(np.outer(S,T),index=S.index,columns=T.index)

def matrix_product(X,Y,strict=False,fillmiss=True):
    """Compute matrix product X@Y, allowing for possibility of missing data.

    The "strict" flag if set requires that the names of levels of indices that vary for columns of X be in the intersection of names of levels of indices that vary for rows of Y.
    """

    if strict and not all(X.columns==Y.index):  # Columns and Indices don't match.
        X.columns = drop_vestigial_levels(X.columns)
        Y.index = drop_vestigial_levels(Y.index)

    if fillmiss:
        X = X.fillna(0)
        Y = Y.fillna(0)

    prod = np.dot(X,Y)

    if len(Y.shape)==1 or Y.shape[1]==1:
        out = pd.Series(prod.squeeze(),index=X.index)
    else:
        out = pd.DataFrame(prod,index=X.index,columns=Y.columns)

    return out

def self_inner(X,min_obs=None):
    """Compute inner product X.T@X, allowing for possibility of missing data."""
    n,m=X.shape

    if n<m:
        axis=1
        N=m
    else:
        axis=0
        N=n

    xbar=X.mean(axis=axis)

    if axis:
        C=(N-1)*X.T.cov(min_periods=min_obs)
    else:
        C=(N-1)*X.cov(min_periods=min_obs)

    return C + N*np.outer(xbar,xbar)

def kron(A,B,sparse=False):
    if sparse: 
        from scipy.sparse import kron

    if isinstance(A,pd.DataFrame):
        a = A.values
        if isinstance(B,pd.DataFrame):
            columns = pd.MultiIndex.from_tuples([(*i,*j) for i in A.columns for j in B.columns])
            b = B.values
        else:
            columns = A.columns.remove_unused_levels()
            b = B.values.reshape((-1,1))
    elif isinstance(B,pd.DataFrame):
        columns = B.columns.remove_unused_levels()
        a = A.values.reshape((-1,1))
        b = B.values

    index = pd.MultiIndex.from_tuples([(*i,*j) for i in A.index for j in B.index],
                                      names=A.index.names+B.index.names)

    if sparse:
        a = kron(a,b)
        return pd.DataFrame.sparse.from_spmatrix(a,columns=columns,index=index)
    else:
        a = np.kron(a,b)
        return pd.DataFrame(a,columns=columns,index=index)
    


#+end_src
*** Binary Operation Tests
#+begin_src python :tangle metrics_miscellany/test/test_index_multiplication.py
import metrics_miscellany.datamat as dm
import pandas as pd
import numpy as np

idx = pd.MultiIndex.from_tuples([(0,0,0),(0,0,1),(1,0,0),(1,0,1)],names=['i','j','k'])
X = dm.DataMat([[1,2,3,4]],columns=idx,idxnames=['l'])
Y = dm.DataMat([[1,2,3,0]],columns=idx.droplevel('j'),idxnames='m').T

assert (X@Y).index.names == ['l']

(X.matmul(Y,strict=True))
#+end_src
#+begin_src python :tangle metrics_miscellany/test/test_binary_ops.py
import metrics_miscellany.datamat as dm
import pandas as pd
import numpy as np

def test_matmul(A,B):
    C = A@B
    assert isinstance(C,type(A))

    return C

def test_matmul_matvec(A,b):
    C = A@b
    assert isinstance(C,type(b))

    return C

if __name__=='__main__':
    A = np.array([[1,2],[3,4]])
    B = np.array([[1,1]]).T
    Cnp = test_matmul(A,B)

    A = pd.DataFrame([[1,2],[3,4]])
    B = pd.DataFrame([[1,1]]).T
    Cpd = test_matmul(A,B)

    A = dm.DataMat([[1,2],[3,4]])
    B = dm.DataMat([[1,1]]).T
    Cdm = test_matmul(A,B)

    b = dm.DataVec([1,1])
    c = test_matmul_matvec(A,b)

    b = pd.Series([1,1])
    c = test_matmul_matvec(A,b)
#+end_src
** Matrix Decompositions
#+begin_src python :tangle metrics_miscellany/utils.py
def heteropca(C,r=1,max_its=50,tol=1e-3,verbose=False):
    """Estimate r factors and factor weights of covariance matrix C."""
    from scipy.spatial import procrustes

    N = C - np.diag(np.diag(C))

    ulast = np.zeros((N.shape[1],r))
    u = np.zeros((N.shape[1],r))
    u[0,0] = 1
    ulast[-1,0] = 1

    t = 0

    while procrustes(u,ulast)[-1] >tol and t<max_its:
        ulast = u

        u,s,vt = np.linalg.svd(N,full_matrices=False,hermitian=True)

        s = s[:r]
        u = u[:,:r]

        Ntilde = u[:,:r]@np.diag(s[:r])@vt[:r,:]

        N = N - np.diag(np.diag(N)) + np.diag(np.diag(Ntilde))

        t += 1

        if t==max_its:
            warnings.warn("Exceeded maximum iterations (%d)" % max_its)
        if verbose: print(f"Iteration {t}, u[0,:r]={u[0,:r]}.")

    return u,s

def svd_missing(A,max_rank=None,min_obs=None,heteroskedastic=False,verbose=False):
    """Singular Value Decomposition with missing values

    Returns matrices U,S,V.T, where A~=U*S*V.T.

    Inputs:
        - A :: matrix or pd.DataFrame, with NaNs for missing data.

        - max_rank :: Truncates the rank of the representation.  Note
                      that this impacts which rows of V will be
                      computed; each row must have at least max_rank
                      non-missing values.  If not supplied rank may be
                      truncated using the Kaiser criterion.

        - min_obs :: Smallest number of non-missing observations for a
                     row of U to be computed.

        - heteroskedastic :: If true, use the "heteroPCA" algorithm
                       developed by Zhang-Cai-Wu (2018) which offers a
                       correction to the svd in the case of
                       heteroskedastic errors.  If supplied as a pair,
                       heteroskedastic[0] gives a maximum number of
                       iterations, while heteroskedastic[1] gives a
                       tolerance for convergence of the algorithm.

    Ethan Ligon                                        September 2021

    """
    # Defaults; modify by passing a tuple to heteroskedastic argument.
    max_its=50
    tol = 1e-3

    P = self_inner(A,min_obs=min_obs) # P = A.T@A

    sigmas,v=np.linalg.eigh(P)

    order=np.argsort(-sigmas)
    sigmas=sigmas[order]

    # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
    v=v[:,order]
    v=v[:,sigmas>0]
    s=np.sqrt(sigmas[sigmas>0])

    if max_rank is not None and len(s) > max_rank:
        v=v[:,:max_rank]
        s=s[:max_rank]

    r=len(s)

    if heteroskedastic: # Interpret tuple
        try:
            max_its,tol = heteroskedastic
        except TypeError:
            pass
        Pbar = P.mean()
        v,s = heteropca(P-Pbar,r=r,max_its=max_its,tol=tol,verbose=verbose)

    if A.shape[0]==A.shape[1]: # Symmetric; v=u
        return v,s,v.T
    else:
        vs=v@np.diag(s)

        u=np.zeros((A.shape[0],len(s)))
        for j in range(A.shape[0]):
            a=A.iloc[j,:].values.reshape((-1,1))
            x=np.nonzero(~np.isnan(a))[0] # non-missing elements of vector a
            if len(x)>=r:
                u[j,:]=(np.linalg.pinv(vs[x,:])@a[x]).reshape(-1)
            else:
                u[j,:]=np.nan

    s = pd.Series(s)
    u = pd.DataFrame(u,index=A.index)
    v = pd.DataFrame(v,index=A.columns)

    return u,s,v
#+end_src
** DataFrame/Mat Manipulations
#+begin_src python :tangle metrics_miscellany/utils.py
from cfe.df_utils import use_indices
from pandas import concat, get_dummies, MultiIndex

def dummies(df,cols,suffix=False):
    """From a dataframe df, construct an array of indicator (dummy) variables,
    with a column for every unique row df[cols]. Note that the list cols can
    include names of levels of multiindices.

    The optional argument =suffix=, if provided as a string, will append suffix
    to column names of dummy variables. If suffix=True, then the string '_d'
    will be appended.
    """
    idxcols = list(set(df.index.names).intersection(cols))
    colcols = list(set(cols).difference(idxcols))

    v = concat([use_indices(df,idxcols),df[colcols]],axis=1)

    usecols = []
    for s in idxcols+colcols:
        usecols.append(v[s].squeeze())

    tuples = pd.Series(list(zip(*usecols)),index=v.index)

    v = get_dummies(tuples).astype(int)

    if suffix==True:
        suffix = '_d'

    if suffix!=False and len(suffix)>0:
        columns = [tuple([str(c)+suffix for c in t]) for t in v.columns]
    else:
        columns = v.columns
        
    v.columns = MultiIndex.from_tuples(columns,names=idxcols+colcols)

    return v
#+end_src
** Index utilities
#+begin_src python :tangle metrics_miscellany/utils.py
import pandas as pd
def drop_vestigial_levels(idx,axis=0,both=False):
    """
    Drop levels that don't vary across the index.

    >>> idx = pd.MultiIndex.from_tuples([(1,1),(1,2)],names=['i','j'])
    >>> drop_vestigial_levels(idx)
    Index([1, 2], dtype='int64', name='j')
    """
    if both:
        return drop_vestigial_levels(drop_vestigial_levels(idx,axis=1))

    if axis==1:
        idx = idx.T

    if isinstance(idx,(pd.DataFrame,pd.Series)):
        df = idx
        idx = df.index
        HumptyDumpty = True
    else:
        HumptyDumpty = False

    try:
        l = 0
        L = len(idx.levels)
        while l < L:
            if len(set(idx.codes[l]))<=1:
                idx = idx.droplevel(l)
                L -= 1
            else:
                l += 1
                if l>=L: break
    except AttributeError:
        pass

    if HumptyDumpty:
        df.index = idx
        idx = df
        if axis==1:
            idx = idx.T

    return idx

#+end_src

* datamat
** DataVec class
#+begin_src python :tangle metrics_miscellany/datamat.py
import pandas as pd
import numpy as np
from metrics_miscellany.utils import matrix_product, diag
from metrics_miscellany.utils import inv as matrix_inv
from metrics_miscellany.utils import pinv as matrix_pinv
import metrics_miscellany.utils as utils
from functools import cached_property
from scipy import sparse as scipy_sparse

class DataVec(pd.Series):
    __pandas_priority__ = 5000

    def __init__(self, *args, **kwargs):
        """Create a DataVec.

        Inherit from :meth: `pd.Series.__init__`.

        Additional Parameters
        ---------------------
        idxnames
                (List of) name(s) for levels of index.
        """
        if 'idxnames' in kwargs.keys():
            idxnames = kwargs.pop('idxnames')
        else:
            idxnames = None

        super(DataVec, self).__init__(*args,**kwargs)

        # Always work with multiindex
        try:
            self.index.levels
        except AttributeError:
            self.index = pd.MultiIndex([self.index],[range(len(self.index))],names=self.index.names)

        if idxnames is None:
            idxnames = list(self.index.names)
            it = 0
            for i,name in enumerate(idxnames):
                if name is None:
                    idxnames[i] = f"_{it:d}"
                    it += 1
        elif isinstance(idxnames,str):
            idxnames = [idxnames]

        self.index.names = idxnames

    def __getitem__(self,key):
        """v.__getitem__(k) == v[k]

        >>> v = DataVec({'a':1,'b':2})
        >>> v['a']
        1
        """
        try:
            return pd.Series.__getitem__(self,key)
        except KeyError: # Perhaps key was for an index?
            return __getitem__(self,(key,))

    @property
    def _constructor(self):
        return DataVec

    @property
    def _constructor_expanddim(self):
        return DataMat

    # Unary operations
    def dg(self,sparse=True):
        """Return"""
        if sparse:
            # We can wind up blowing ram if not careful...
            d = scipy_sparse.diags(self.values)
            return DataMat(pd.DataFrame.sparse.from_spmatrix(d,index=self.index,columns=self.index))
        else:
            return DataMat(np.diag(self.values),index=self.index,columns=self.index)

    def norm(self,ord=None,**kwargs):
        return np.linalg.norm(self,ord,**kwargs)

    # Binary operations
    def outer(self,other):
        """Outer product of two series (vectors).
        """
        return DataMat(np.outer(self,other),index=self.index,columns=other.index)

    # Other manipulations
    def concat(self,other,axis=0,levelnames=False,toplevelname='v',suffixer='_',
               drop_vestigial_levels=False,**kwargs):
        p = DataMat(self)
        out = p.concat(other,axis=axis,
                       levelnames=levelnames,
                       toplevelname=toplevelname,
                       suffixer=suffixer,
                       drop_vestigial_levels=drop_vestigial_levels,
                       ,**kwargs)

        if axis==0: out = out.squeeze()

        return out


#+end_src
** DataMat class
#+begin_src python :tangle metrics_miscellany/datamat.py

class DataMat(pd.DataFrame):
    __pandas_priority__ = 6000

    def __init__(self, *args, **kwargs):
        """Create a DataMat.

        Inherit from :meth: `pd.DataFrame.__init__`.

        Additional Parameters
        ---------------------
        idxnames
                (List of) name(s) for levels of index.
        colnames
                (List of) name(s) for levels of columns.
        name
                String naming DataMat object.
        """
        if 'idxnames' in kwargs.keys():
            idxnames = kwargs.pop('idxnames')
        else:
            idxnames = None

        if 'colnames' in kwargs.keys():
            colnames = kwargs.pop('colnames')
        else:
            colnames = None

        if 'name' in kwargs.keys():
            name = kwargs.pop('name')
        else:
            name = None

        super(DataMat, self).__init__(*args,**kwargs)

        self.name = name

        # Always work with multiindex
        try:
            self.index.levels
        except AttributeError:
            self.index = pd.MultiIndex([self.index],[range(len(self.index))],names=self.index.names)

        try:
            self.columns.levels
        except AttributeError:
            self.columns = pd.MultiIndex([self.columns],[range(len(self.columns))],names=self.columns.names)


        if idxnames is None:
            idxnames = list(self.index.names)
            it = 0
            for i,name in enumerate(idxnames):
                if name is None:
                    idxnames[i] = f"_{it:d}"
                    it += 1
        elif isinstance(idxnames,str):
            idxnames = [idxnames]

        self.index.names = idxnames

        if colnames is None:
            colnames = list(self.columns.names)
            it = 0
            for i,name in enumerate(colnames):
                if name is None:
                    colnames[i] = f"_{it:d}"
                    it += 1
        elif isinstance(colnames,str):
            colnames = [colnames]

        self.columns.names = colnames

    def __getitem__(self,key):
        """X.__getitem__(k) == X[k]

        >>> X = DataMat([[1,2,3],[4,5,6]],colnames='cols',idxnames='rows')
        >>> X[0].sum().squeeze()==5
        True
        """
        try:
            return pd.DataFrame.__getitem__(self,key)
        except KeyError: # Perhaps key was for an index?
            return __getitem__(self,(key,))

    @property
    def _constructor(self):
        return DataMat

    @property
    def _constructor_sliced(self):
        return DataVec

    def stack(self,**kwargs):
        if 'future_stack' in kwargs.keys():
            return pd.DataFrame.stack(self,**kwargs)
        else:
            return pd.DataFrame.stack(self,future_stack=True,**kwargs)

    # Unary operations
    @cached_property
    def inv(self):
        return DataMat(matrix_inv(self))

    @cached_property
    def norm(self,ord=None,**kwargs):
        return np.linalg.norm(self,ord,**kwargs)

    def dg(self):
        """Extract diagonal from square matrix.

        >>> DataMat([[1,2],[3,4]],idxnames='i').dg().values.tolist()
        [1, 4]
        """
        assert np.all(self.index==self.columns), "Should have columns same as index."
        return DataVec(np.diag(self.values),index=self.index)

    def rank(self,**kwargs):
        """Matrix rank"""
        return np.linalg.matrix_rank(self,**kwargs)

    def svd(self,hermitian=False):
        """Singular value composition into U@S.dg@V.T."""
        idx = self.index
        cols = self.columns
        u,s,vt = np.linalg.svd(self,compute_uv=True,full_matrices=False,hermitian=hermitian)
        u = DataMat(u,index=idx)
        vt = DataMat(vt,columns=cols)
        s = DataVec(s)

        return u,s,vt

    @cached_property
    def pinv(self):
        """Moore-Penrose pseudo-inverse."""
        return DataMat(matrix_pinv(self))

    # Binary operations
    def matmul(self,other,strict=False):
        Y = matrix_product(self,other,strict=strict)
        if len(Y.shape) <= 1 or Y.shape[1]==1:
            return DataVec(Y)
        else:
            return DataMat(Y)

    __matmul__ = matmul

    def kron(self,other,sparse=False):
        return DataMat(utils.kron(self,other,sparse=sparse))

    def lstsq(self,other):
        rslt = np.linalg.lstsq(self,other,rcond=None)

        if len(rslt[0].shape)<2 or rslt[0].shape[1]==1:
            b = DataVec(rslt[0],index=self.columns)
        else:
            b = DataMat(rslt[0],index=self.columns,columns=other.columns)

        return b

    def proj(self,other):
        b = other.lstsq(self)
        return other@b

    # Other transformations
    def dummies(self,cols,suffix=''):
        return DataMat(utils.dummies(self,cols,suffix=suffix))

    def concat(self,other,axis=0,levelnames=False,toplevelname='v',suffixer='_',
               drop_vestigial_levels=False,**kwargs):
        """Concatenate self and other.

        This uses the machinery of pandas.concat, but ensures that when two
        DataMats having multiindices with different number of levels are
        concatenated that new levels are added so as to preserve a result with a
        multiindex.

        if other is a dictionary and levelnames is not False, then a new level in the multiindex is created naming the columns belonging to the original DataMats.

        USAGE
        -----
        >>> a = DataVec([1,2],name='a',idxnames='i')
        >>> b = DataMat([[1,2],[3,4]],name='b',idxnames='i',colnames='j')
        >>> b.concat([a,b],axis=1,levelnames=True).columns.levels[0].tolist()
        ['b', 'a', 'b_0']
        """
        # Make other a list, unless it's a dict, and get allnames.
        if levelnames==False:
            assign_missing=True
        else:
            assign_missing=levelnames
            levelnames = True

        allobjs = []
        if isinstance(other,dict):
            allobjs = [self] + list(other.values())
            allnames = [self.name] + list(other.keys())
        else:
            if isinstance(other,tuple):
                allobjs = [self] + list(other)
            elif isinstance(other,(DataMat,DataVec)):
                allobjs = [self,other]
                allnames = [self.name] + get_names([other],assign_missing=assign_missing)
            elif isinstance(other,list):
                allobjs = [self]+other
            else:
                raise ValueError("Unexpected type")

            allnames = get_names(allobjs,assign_missing=assign_missing)

        # Have list of all names, but may not be unique.

        suffix = (f'{suffixer}{i:d}' for i in range(len(allnames)))
        unique_names = []
        for i,name in enumerate(allnames):
            if name is None:
                name = next(suffix)
            if name not in unique_names:
                unique_names.append(name)
            else:
                unique_names.append(name+next(suffix))

        # Reconcile indices so they all have common named levels.
        idxs = reconcile_indices([obj.index for obj in allobjs],
                                 drop_vestigial_levels=drop_vestigial_levels)
        for i in range(len(idxs)):
            allobjs[i].index = idxs[i]

        # Get list of columns, allowing for DataVec
        allcols = []
        for i,obj in enumerate(allobjs):
            try:
                allcols += [obj.columns]
            except AttributeError: # No columns attribute?
                obj = DataMat(obj)
                allobjs[i] = obj
                allcols += [obj.columns]
        cols = reconcile_indices(allcols,drop_vestigial_levels=drop_vestigial_levels)
        for i in range(len(idxs)):
            allobjs[i].columns = cols[i]

        # Now have a list of unique names, build a dictionary
        d = dict(zip(unique_names,allobjs))

        if levelnames:
            return utils.concat(d,axis=axis,names=toplevelname,**kwargs)
        else:
            return utils.concat(allobjs,axis=axis,**kwargs)
#+end_src

** datamat utils
#+begin_src python :tangle metrics_miscellany/datamat.py
def get_names(dms,assign_missing=False):
    """
    Given an iterable of DataMats or DataVecs, return a list of names.

    If an item does not have a name, give "None" unless assign_missing,
    in which case:

       assign_missing==True: use a sequence "_0", "_1", etc.
       assign_missing is a list: Use this list to assign names.

    >>> a = DataVec([1,2],name='a')
    >>> b = DataMat([[1,2]],name='b')
    >>> c = DataMat([[1,2]])

    >>> get_names([a,b,c])
    ['a', 'b', None]

    >>> get_names([a,b,c],assign_missing=True)
    ['a', 'b', '_0']
    """
    names = []
    for item in dms:
        try:
            names += [item.name]
        except AttributeError:
            names += [None]

    if not assign_missing: return names
    else:
        if assign_missing==True:
            missnames = (f'_{i:d}' for i in range(len(names)))
        else:
            missnames = (name for name in assign_missing)

        for i,item in enumerate(names):
            if item is None:
                names[i] = next(missnames)
        return names

def reconcile_indices(idxs,fillvalue='',drop_vestigial_levels=False):
    """
    Given a list of indices, give them all the same levels.

    >>> idx0 = pd.MultiIndex
    """
    # Get union of index level names, preserving order
    names = []
    dropped_level_values = []
    newidxs = []
    for x in idxs:
        # Identify vestigial levels & drop
        droppednames = {}
        for i,level in enumerate(x.levels):
            if drop_vestigial_levels and len(level)==1: # Vestigial level
                try:
                    if len(x.levels)>1:
                        dropname = x.names[i]
                        x = x.droplevel(dropname)
                        droppednames[dropname] = level[0]
                except AttributeError: # May be an index
                    pass
        dropped_level_values.append(droppednames)
        newidxs.append(x)
        for newname in x.names:
            if newname not in names:
                names += [newname]

    # Add levels to indices where necessary
    out = []
    for i,idx in enumerate(newidxs):
        for levelname in names:
            if levelname not in idx.names:
                droppednames = dropped_level_values[i]
                try:
                    fillvalue = droppednames[levelname]
                except KeyError: pass
                idx = utils.concat([DataMat(index=idx)],keys=[fillvalue],names=[levelname]).index
        try: # Duck-typing: Is this an index?
            idx.levels
        except AttributeError:
            idx = pd.MultiIndex([idx],[range(len(idx))],names=idx.names)

        out.append(idx.reorder_levels(names))

    return out

def concat(dms,axis=0,levelnames=False,toplevelname='v',suffixer='_',**kwargs):
    """Concatenate self and other.

    This uses the machinery of pandas.concat, but ensures that when two
    DataMats having multiindices with different number of levels are
    concatenated that new levels are added so as to preserve a result with a
    multiindex.

    if other is a dictionary and levelnames is not False, then a new level in the multiindex is created naming the columns belonging to the original DataMats.

    USAGE
    -----
    >>> a = DataVec([1,2],name='a',idxnames='i')
    >>> b = DataMat([[1,2],[3,4]],name='b',idxnames='i',colnames='j')
    >>> concat([a,b],axis=1,levelnames=True).columns.levels[0].tolist()
    ['b', 'a', 'b_0']
    """

    # Make dms a list, unless it's a dict, and get allnames.
    if levelnames==False:
        assign_missing=True
    else:
        assign_missing=levelnames
        levelnames = True

    allobjs = []
    if isinstance(dms,dict):
        allobjs = list(dms.values())
        allnames = list(dms.keys())
    else:
        if isinstance(dms,tuple):
            allobjs = list(dms)
        elif isinstance(dms,(DataMat,DataVec)):
            allobjs = [dms]
            allnames = get_names([dms],assign_missing=assign_missing)
        elif isinstance(dms,list):
            allobjs = dms
        else:
            raise ValueError("Unexpected type")

        allnames = get_names(allobjs,assign_missing=assign_missing)

    # Have list of all names, but may not be unique.

    suffix = (f'{suffixer}{i:d}' for i in range(len(allnames)))
    unique_names = []
    for i,name in enumerate(allnames):
        if name is None:
            name = next(suffix)
        if name not in unique_names:
            unique_names.append(name)
        else:
            unique_names.append(name+next(suffix))

    # Reconcile indices so they all have common named levels.
    idxs = reconcile_indices([obj.index for obj in allobjs])
    for i in range(len(idxs)):
        allobjs[i].index = idxs[i]

    # Get list of columns, allowing for DataVec
    allcols = []
    for i,obj in enumerate(allobjs):
        try:
            allcols += [obj.columns]
        except AttributeError: # No columns attribute?
            obj = DataMat(obj)
            allobjs[i] = obj
            allcols += [obj.columns]
    cols = reconcile_indices(allcols)
    for i in range(len(idxs)):
        allobjs[i].columns = cols[i]

    # Now have a list of unique names, build a dictionary
    d = dict(zip(unique_names,allobjs))

    if levelnames:
        return utils.concat(d,axis=axis,names=toplevelname,**kwargs)
    else:
        return utils.concat(allobjs,axis=axis,**kwargs)


if __name__ == "__main__":
    a = DataVec([1,2],name='a',idxnames='i')
    b = DataMat([[1,2]],name='b',idxnames='i',colnames='j')
    c = DataMat([[1,2]],colnames='k')
    d = c.concat([a,b],levelnames=True,axis=1)

    import doctest
    doctest.testmod()


#+end_src


* Estimators
** Preliminaries
#+begin_src python :tangle metrics_miscellany/estimators.py
import statsmodels.api as sm
from statsmodels.stats import correlation_tools
import numpy as np
from numpy.linalg import lstsq
import warnings
import pandas as pd
from . import gmm
from . GMM_class import GMM
from . import utils
from .datamat import DataMat, DataVec
#+end_src
** OLS
#+begin_src python :tangle metrics_miscellany/estimators.py

def ols(X,y,cov_type='HC3',PSD_COV=False):
    """OLS estimator of b in y = Xb + u.

    Returns both estimate b as well as an estimate of Var(b).

    The estimator used for the covariance matrix depends on the
    optional argument =cov_type=.

    If optional flag PSD_COV is set, then an effort is made to ensure that
    the estimated covariance matrix is positive semi-definite.  If PSD_COV is
    set to a positive float, then this will be taken to be the smallest eigenvalue
    of the 'corrected' matrix.
    """
    n,k = X.shape

    est = sm.OLS(y,X).fit()
    b = pd.DataFrame({'Coefficients':est.params.values},index=X.columns)
    if cov_type=='HC3':
        V = est.cov_HC3
    elif cov_type=='OLS':
        XX = X.T@X
        if np.linalg.eigh(XX)[0].min()<0:
            XX = correlation_tools.cov_nearest(XX,method='nearest')
            warnings.warn("X'X not positive (semi-) definite.  Correcting!  Estimated variances should not be affected.")
        V = est.resid.var()*np.linalg.inv(XX)
    elif cov_type=='HC2':
        V = est.cov_HC2
    elif cov_type=='HC1':
        V = est.cov_HC1
    elif cov_type=='HC0':
        V = est.cov_HC0
    else:
        raise ValueError("Unknown type of covariance matrix.")

    if PSD_COV:
        if PSD_COV is True:
            PSD_COV = (b**2).min()
        s,U = np.linalg.eigh((V+V.T)/2)
        if s.min()<PSD_COV:
            oldV = V
            V = U@np.diag(np.maximum(s,PSD_COV))@U.T
            warnings.warn("Estimated covariance matrix not positive (semi-) definite.\nCorrecting! Norm of difference is %g." % np.linalg.norm(oldV-V))

    V = pd.DataFrame(V,index=X.columns,columns=X.columns)

    return b,V

#+end_src

*** OLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_ols.py
import pandas as pd
from metrics_miscellany.estimators import ols
import numpy as np

def test_ols(N=500000,tol=1e-2):

    x = pd.DataFrame({'x':np.random.standard_normal((N,))})
    x['Constant'] = 1

    beta = pd.DataFrame({'Coefficients':[1,0]},index=['x','Constant'])

    u = pd.DataFrame(np.random.standard_normal((N,))/10)

    y = (x@beta).values + u.values
    b,V = ols(x,y)

    assert np.allclose(b,beta,atol=tol)

if __name__=='__main__':
    test_ols()

#+end_src
** Jrue OLS
Jrue's implementation of OLS, no dependency on statsmodels.
#+begin_src python :tangle metrics_miscellany/estimators.py
def jrue_ols(X,y):

    b = lstsq(X.T@X,X.T@y)

    b = pd.Series(b.squeeze(),index=X.columns)

    e = y.squeeze() - X@b

    V = e.var()*utils.inv(X.T@X)

    return b,V
#+end_src
** Two-stage Least Squares
#+begin_src python :tangle metrics_miscellany/estimators.py
def tsls(X,y,Z,return_Omega=False):
    """
    Two-stage least squares estimator.
    """

    n,k = X.shape

    Qxz = X.T@Z/n

    zzinv = utils.inv(Z.T@Z/n)
    b = lstsq(Qxz@zzinv@Qxz.T,Qxz@zzinv@Z.T@y/n,rcond=None)[0]

    b = pd.Series(b.squeeze(),index=X.columns)

    # Cov matrix
    e = y.squeeze() - X@b

    #Omega = Z.T@(e**2).dg()@Z/n
    # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
    ZTe = Z.T.multiply(e)
    Omega = ZTe@ZTe.T/n

    #Omega = pd.DataFrame(e.var()*Z.T.values@Z.values/n,columns=Z.columns,index=Z.columns)

    if return_Omega:
        return b,Omega
    else:
        A = (Qxz@zzinv@Qxz.T).inv
        V = A@(Qxz@zzinv@Omega@zzinv@Qxz.T)@A.T/n
        return b,V

#+end_src
*** TSLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_tsls.py
import pandas as pd
from metrics_miscellany.estimators import tsls, ols
from metrics_miscellany.datamat import DataMat, DataVec
import numpy as np

def test_tsls(N=500000,tol=1e-2):

    z = DataMat({'z':np.random.standard_normal((N,))})
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.squeeze() + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    #b,V = tsls(x,y,z)
    b,V = ols(x,y)

    assert np.allclose(b,beta.squeeze(),atol=tol)

if __name__=='__main__':
    test_tsls()

#+end_src

** Linear GMM
#+begin_src python :tangle metrics_miscellany/estimators.py
def linear_gmm(X,y,Z,W=None,return_Omega=False):
    """
    Linear GMM estimator.
    """

    if W is None: # Use 2sls to get initial estimate of W
        b1,Omega1 = tsls(X,y,Z,return_Omega=True)
        W = Omega1.inv
        return linear_gmm(X,y,Z,W=W)
    else:
        n,k = X.shape

        Qxz = X.T@Z/n

        b = lstsq(Qxz@W@Qxz.T,Qxz@W@Z.T@y/n,rcond=None)[0]

        b = pd.Series(b.squeeze(),index=X.columns)

        # Cov matrix
        e = y.squeeze() - X@b

        #Omega = Z.T@(e**2).dg()@Z/n
        # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
        ZTe = Z.T.multiply(e)
        Omega = ZTe@ZTe.T/n

        if return_Omega:
            return b,Omega
        else:
            Vb = (Qxz@Omega.inv@Qxz.T).inv/n
            return b,Vb

#+end_src
*** Linear GMM Tests
#+begin_src python :tangle metrics_miscellany/test/test_linear_gmm.py
import pandas as pd
from metrics_miscellany.estimators import linear_gmm
from metrics_miscellany.datamat import DataMat, DataVec
import numpy as np

def test_linear_gmm(N=500000,tol=1e-2):

    z = DataMat(np.random.standard_normal((N,2)),columns=['z1','z2'])
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.sum(axis=1) + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    b,V = linear_gmm(x,y,z)

    assert np.allclose(b,beta.squeeze(),atol=tol)

    return b,V

if __name__=='__main__':
    b,V = test_linear_gmm()

#+end_src

** Linear GMM with Linear Restrictions
#+begin_src python :tangle metrics_miscellany/estimators.py
def restricted_linear_gmm(X,y,Z,R,r,W=None,return_Omega=False):
    """
    Linear GMM estimator.
    """
    raise NotImplementedError
    if W is None: # Use 2sls to get initial estimate of W
        b1,Omega1 = tsls(X,y,Z,return_Omega=True)
        W = Omega1.inv
        return linear_gmm(X,y,Z,W=W)
    else:
        n,k = X.shape

        Qxz = X.T@Z/n

        b = lstsq(Qxz@W@Qxz.T,Qxz@W@Z.T@y/n,rcond=None)[0]

        b = pd.Series(b.squeeze(),index=X.columns)

        # Cov matrix
        e = y.squeeze() - X@b

        #Omega = Z.T@(e**2).dg()@Z/n
        # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
        ZTe = Z.T.multiply(e)
        Omega = ZTe@ZTe.T/n

        if return_Omega:
            return b,Omega
        else:
            Vb = (Qxz@Omega.inv@Qxz.T).inv/n
            return b,Vb

#+end_src
*** Linear GMM Tests
#+begin_src python :tangle metrics_miscellany/test/test_linear_gmm.py
import pandas as pd
from metrics_miscellany.estimators import linear_gmm
from metrics_miscellany.datamat import DataMat, DataVec
import numpy as np

def test_linear_gmm(N=500000,tol=1e-2):

    z = DataMat(np.random.standard_normal((N,2)),columns=['z1','z2'])
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.sum(axis=1) + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    b,V = linear_gmm(x,y,z)

    assert np.allclose(b,beta.squeeze(),atol=tol)

    return b,V

if __name__=='__main__':
    b,V = test_linear_gmm()

#+end_src

** Factor Regression
We're interested here in multivariate regressions of the form
\[
     Y = XB + F\Lambda + U
\]
where $Y$ and $U$ are $N\times k$, $X$ is $n\times \ell$, $B$ is $\ell\times k$, $F$ is $N\times r$, and \Lambda is $r\times k$.  Only $(Y,X)$ are observed; $F$ is a collection of latent "factors."  The identifying assumptions are that $U$ is orthogonal to $(X,F)$ and $\E F_iF_i^\T = I_r$.  [cite/t:@hansen20:econometrics] describes an iterative approach to estimation, which we implement below.
#+begin_src python :tangle metrics_miscellany/estimators.py
def factor_regression(Y,X,F=None,rank=1,tol=1e-3):

    if rank>1:
        raise NotImplementedError("Factor regression for rank>1 is not reliable.")

    N,k = Y.shape
    def ols(X,Y):
        N,k = Y.shape
        XX = utils.self_inner(X)/N
        XY = utils.matrix_product(X.T,Y)/N
        B = np.linalg.lstsq(XX,XY,rcond=None)[0]
        return pd.DataFrame(B,index=X.columns,columns=Y.columns)

    if F is None:
        B = ols(X,Y)
        F = 0
    else:
        parms = ols(pd.concat([X,F],axis=1),Y)
        L = parms.iloc[-rank:,:]
        B = parms.iloc[:-rank,:]

    lastF = F
    F,s,vt = utils.svd_missing(Y - utils.matrix_product(X,B),max_rank=rank)
    scale = F.std()
    F = F.multiply(1/scale)

    if np.linalg.norm(F-lastF)>tol:
        B,L,F = factor_regression(Y,X,F=F,rank=rank,tol=tol)

    return B,L,F


#+end_src
*** Factor Regression Test
#+begin_src python :tangle metrics_miscellany/test/test_factor_regression.py
import pandas as pd
from scipy import stats
from metrics_miscellany.estimators import factor_regression
from metrics_miscellany import utils
import numpy as np

def generate_multivariate_normal(N,k,V=None,colidx='a'):

    try:
        a = ord(colidx)
        labels = list(map(chr, range(a, a+k)))
    except TypeError:
        labels = range(colidx,colidx+k)

    if V is None:
        D = pd.DataFrame(np.random.randn(k,k),index=labels,columns=labels)
        V = D.T@D
    else:
        V = pd.DataFrame(V,index=labels,columns=labels)

    X = pd.DataFrame(stats.multivariate_normal(cov=V).rvs(N),columns=labels)

    return X

def main(N,k,l,r):

    U = generate_multivariate_normal(N,k,V=np.eye(k),colidx='A')/100

    X = generate_multivariate_normal(N,l)

    a = ord('a')
    A = ord('A')
    rlabels = list(map(chr, range(a, a+l)))
    clabels = list(map(chr, range(A, A+k)))
    B = pd.DataFrame(np.arange(1,l*k+1).reshape(l,k),index=rlabels,columns=clabels)

    F = generate_multivariate_normal(N,r,colidx=0)
    F = F - F.mean()
    scale = F.std()
    F = F.multiply(1/scale)

    L = pd.DataFrame(np.arange(1,k*r+1).reshape(r,k)/10,index=F.columns,columns=U.columns)
    L = L.multiply(scale,axis=0)

    Y = utils.matrix_product(X,B) + utils.matrix_product(F,L) + U

    return Y,X,F,B,L,U

def test_factor_regression(N=1000,k=10,l=2,r=1):
    Y,X,F0,B0,L0,U0 = main(N,k,l,r)
    X['Constant'] = 1

    B,L,F = factor_regression(Y,X,rank=r)

    assert np.linalg.norm(F0-F) < np.linalg.norm(F0)

    assert np.all(Y.var()>(Y-X@B).var())

    assert np.all((Y-X@B).var()>(Y-X@B-F@L).var())

    assert np.linalg.norm((B0-B).dropna())/np.linalg.norm(B0) < 0.01

if __name__ == '__main__':
    test_factor_regression(N=10000,r=1)
 #+end_src

* GMM
** Procedural interface for GMM estimator.
#+begin_src python :tangle metrics_miscellany/gmm.py
import numpy as np
from . import utils
from . import utils
matrix_product = utils.matrix_product
diag = utils.diag
inv = utils.inv

from scipy.optimize import minimize_scalar, minimize, approx_fprime
from scipy.optimize import minimize as scipy_min
import pandas as pd

from IPython.core.debugger import Pdb

__version__ = "0.3.1"

######################################################
# Beginning of procedural version of gmm routines

def gN(b,data):
    """Averages of g_j(b,data).

    This is generic for data, to be passed to gj.

    Parameters
    ----------
    b:numpy array, the initial value of b0
    data: tuple, the data we'll input to estimate b0.

    Returns
    -------
    Sample moments @ b
    """
    e = gj(b,data)

    gN.N,gN.k = e.shape

    if hasattr(e,'count'):
        gN.N = e.count()  # Allows for possibility of missing data
    elif isinstance(e,np.ndarray):  # There is no 'count' method in ndarray, we need another method
        gN.N = np.count_nonzero(~np.isnan(e))
    else:
        pass

    # Check to see more obs. than moments.
    assert np.all(gN.N > gN.k), "More moments than observations"

    try:
        return e.mean(axis=0).reshape((-1,1))
    except AttributeError:
        return e.mean(axis=0)

def Omegahat(b,data):
    e = gj(b,data)

    # Recenter! We have Eu=0 under null.
    # Important to use this information.
    e = e - e.mean(axis=0)

    if hasattr(e,'count'):
        sqrtN = np.sqrt(e.count())
    elif isinstance(e,np.ndarray):
        sqrtN = np.sqrt(np.count_nonzero(~np.isnan(e)))

    e = e/sqrtN

    ete = matrix_product(e.T,e)

    return ete

def JN(b,data,W=None):

    if W is None:
        W = utils.inv(Omegahat(b,data))

    m = gN(b,data) # Sample moments @ b

    #Pdb().set_trace()

    # Scaling by diag(N) allows us to deal with missing values
    WN = pd.DataFrame(matrix_product(diag(gN.N),W))

    crit = (m.T@WN@m).squeeze()
    assert crit >= 0

    return crit

def minimize(f,b_init=None):

    if b_init is None:
        return minimize_scalar(f).x
    else:
        b_init = b_init.ravel() # ensure the dimension fulfill the requirement
        return scipy_min(f,b_init).x

def one_step_gmm(data,W=None,b_init=None):

    if b_init is None:
        b_init = np.zeros(data.shape[1])

    if W is None:
        e = gj(b_init,data)
        if isinstance(e,pd.DataFrame):
            W = pd.DataFrame(np.eye(e.shape[1]),index=e.columns,columns=e.columns)
        else:
            W = utils.inv(Omegahat(b_init,data))

    assert np.linalg.matrix_rank(W)==W.shape[0]

    b = minimize(lambda b: JN(b,data,W),b_init=b_init)

    return b, JN(b,data,W)

def two_step_gmm(data,b_init=None):

    # First step uses identity weighting matrix
    b1 = one_step_gmm(data,b_init=b_init)[0]

    # Construct 2nd step weighting matrix using
    # first step estimate of beta
    W2 = utils.inv(Omegahat(b1,data))

    return one_step_gmm(data,W=W2,b_init=b1)

def continuously_updated_gmm(data,b_init=None):

    if b_init is None:
        b_init = np.zeros(data.shape[1])

    # First step uses identity weighting matrix
    W = utils.inv(Omegahat(b_init,data))

    bhat = minimize(lambda b: JN(b,data,utils.inv(Omegahat(b,data))),b_init=b_init)

    return bhat, JN(bhat,data,utils.inv(Omegahat(bhat,data)))

def dgN(b):
    """
    Average gradient of gj(b).

    This function provides numerical derivatives.  

    One may wish to override this with a function dgN which returns analytical derivatives.
    """
    gradient = pd.DataFrame(approx_fprime(b,gN),index=gN(b).index)

    return gradient
    

def Vb(b):
    """Covariance of estimator of b.

    Note that one must supply gmm.dgN, the average gradient of gmm.gj at b.
    """
    Q = dgN(b)
    W = pd.DataFrame(matrix_product(diag(gN.N),Omegahat(b)))

    return utils.inv(Q.T@utils.inv(W)@Q)

def print_version():
    print(__version__)

# End of procedural version of gmm routines
######################################################
#+end_src
*** GMM Test
#+begin_src python :tangle metrics_miscellany/test/test_gmm.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import gmm
from numpy.linalg import inv
from scipy.stats import distributions as iid

def dgp(N,beta,gamma,sigma_u,VXZ):
    """Generate a tuple of (y,X,Z).

    Satisfies model:
        y = X@beta + u
        E Z'u = 0
        Var(u) = sigma^2
        Cov(X,u) = gamma*sigma_u^2
        Var([X,Z]|u) = VXZ
        u,X,Z mean zero, Gaussian

    Each element of the tuple is an array of N observations.

    Inputs include
    - beta :: the coefficient of interest
    - gamma :: linear effect of disturbance on X
    - sigma_u :: Variance of disturbance
    - VXZ :: Var([X,Z]|u)
    """

    u = pd.Series(iid.norm.rvs(size=(N,))*sigma_u)

    # "Square root" of VXZ via eigendecomposition
    lbda,v = np.linalg.eig(VXZ)
    SXZ = v@np.diag(np.sqrt(lbda))

    # Generate normal random variates [X*,Z]
    XZ = pd.DataFrame(iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T)

    # But X is endogenous...
    X = XZ.loc[:,0].add(gamma*u,axis=0)
    Z = XZ.loc[:,1:]

    # Calculate y
    y = X*beta + u

    return y,X,Z

def test_gmm(N=10000):

    ## Play with us!
    beta = 1     # "Coefficient of interest"
    gamma = 1    # Governs effect of u on X
    sigma_u = 1  # Note assumption of homoskedasticity
    ## Play with us!

    # Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ

    ell = 4 # Play with me too!

    # Arbitrary (but deterministic) choice for VXZ = [VX Cov(X,Z);
    #                                                 Cov(Z,X) VZ]
    # Pinned down by choice of a matrix A...
    A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1))

    ## Below here we're less playful.

    # Now Var([X,Z]|u) is constructed so guaranteed pos. def.
    VXZ = A.T@A

    Q = -VXZ[1:,[0]]  # -EZX', or generally Edgj/db'

    # Gimme some truth:
    truth = (beta,gamma,sigma_u,VXZ)

    ## But play with Omega if you want to introduce heteroskedascity
    Omega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')

    # Asymptotic variance of optimally weighted GMM estimator:
    AVar_b = inv(Q.T@inv(Omega)@Q)

    data = dgp(N,*truth)

    def gj(b):
        y,X,Z = data
        e = (y.squeeze()-b*X.squeeze())

        Ze = Z.multiply(e,axis=0)

        return Ze

    def dgN(b):
        y,X,Z = data
        return Z.T@X

    gmm.gj = gj
    gmm.dgN = dgN

    b,J = gmm.two_step_gmm()

    assert (b-beta)**2 < 0.01, f"Estimate {b} outside tolerance."

    print(b,J)
    print(gmm.Vb(b))

    return J

if __name__ == '__main__':
    J = []
    for i in range(1000): J.append(test_gmm())

#+end_src
** GMM Class
#+begin_src python :tangle metrics_miscellany/GMM_class.py
from . import gmm
import numpy as np

class GMM(object):

    def __init__(self,gj,data,B,W=None):
        """GMM problem for restrictions E(gj(b0))=0, estimated using data with b0 in R^k.

           - If supplied B is a positive integer k, then
             space taken to be R^k.
           - If supplied B is a k-vector, then
             parameter space taken to be R^k with B a possible
             starting value for optimization.

	     Parameters
	     ----------
	     gj: function, the condition moment of b0.
	     data: tuple, the data we'll input to estimate b0.
	     B:
	     if B is an positive integer, then it represents the number of parameters to be estimated in GMM,
	     if B is any vector forms, including tuple, list or array, it represents the initial value of the parameters.
	     W: (optional) the weighting matrix,

	     usually it must satisfy the character of positive definiteness, symmetry and asymptotically efficiency.
	     In most circumstances, we'll use the inverse of the covariance matrix of the moment conditions or the identity matrix as the weighting matrix.
	     Returns
	     ----------
	     GMM class
	     Examples
	     ----------
	     >>>def gj(b,data):
	            '''
		    input:
		        b:numpy array, the initial value of b0
			data:tuple, the data we'll input to estimate b0
		    output:
		        the result of condition moment
		    '''
		    Y,X,Z = data
		    b = b.reshape(1,-1)
		    return utils.dot(Z,utils.sub(Y,X@b.T),'Y')

	     >>>Y = np.array([[1,2,3,4]]).T
	     array([[1],
	            [2],
                    [3],
                    [4]])
	     >>>X = np.array([[2,4,5,7],[4,5,6,7]]).T
             array([[2, 4],
                    [4, 5],
                    [5, 6],
                    [7, 7]])
	     >>>Z = np.array([[3,4,5,6]]).T
	     array([[3],
                    [4],
                    [5],
                    [6]])
             >>>gmm_instance =GMM_class.GMM(gj,(Y,X,Z),2)
             
             Average of sample moment condition
	     >>>gmm_instance.gN()
             banana    12.5
             pear      15.0

             >>>gmm_instance.Omegahat()
                     banana  pear
             banana   62.25  71.0
             pear     71.00  81.0
             
             >>>gmm_instance.JN()
             119.99999999996635

             >>>gmm_instance.one_step_gmm()
             array([ 0.69776498, -0.11635622])

             >>>gmm_instance.two_step_gmm()
             array([ 0.69767442, -0.11627908])

             >>>bhat = gmm_instance.two_step_gmm()
             >>>X@bhat.T
             0    0.930233
             1    2.209302
             2    2.790698
             3    4.069767

        """
        self.gj = gj
        gmm.gj = gj  # Overwrite member of gmm module
        self.data = data

        self.W = W

        self.b = None 

        try:
            self.k = len(B)
            self.b_init = np.array(B).reshape(1,-1)
        except TypeError: # if B is an integer instead of a vector
            self.k = B
            self.b_init = np.zeros(self.k).reshape(1,-1)

        self.ell = gj(self.b_init,self.data).shape[1]

        if type(data) is tuple:
            self.N = data[0].shape[0]
        else:
            self.N = data.shape[0]

        self.minimize = gmm.minimize

    def gN(self,b=None):
        """Averages of g_j(b).

        This is generic for data, to be passed to gj.

        You can input None.

        """
        if b is None:
            b = self.b_init.reshape(1,-1)
        return gmm.gN(b,self.data)

    def Omegahat(self,b=None):
        if b is None:
            b = self.b_init.reshape(1,-1)

        return gmm.Omegahat(b,self.data)

    def JN(self,b=None,W=None):
        if b is None:
            b = self.b_init.reshape(1,-1)

        return gmm.JN(b,self.data,W)

    def one_step_gmm(self,W=None,b_init=None):
        if b_init is None:
            self.b = gmm.one_step_gmm(self.data,W,b_init=self.b_init)[0]
        else:
            self.b = gmm.one_step_gmm(self.data,W,b_init)[0]

        return self.b

    def two_step_gmm(self):

        self.b = gmm.two_step_gmm(self.data,b_init=self.b_init)[0]
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b

    def continuously_updated_gmm(self):

        est = gmm.continuously_updated_gmm(self.data,b_init=self.b_init)[0]
        self.b = est
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b



if __name__=='__main__':
    #foo = GMM(gmm.gj,
    pass

#+end_src

* Hypothesis Tests
** Chi square tests
#+begin_src python :tangle metrics_miscellany/tests.py
from metrics_miscellany import utils
from scipy import stats
import pandas as pd
import numpy as np

def chi2_test(b,V,var_selection=None,R=None,TEST=False):
    """Construct chi2 test of R'b = 0.

    If R is None then test is b = 0.

    If one wishes to test a hypothesis regarding only a subset of elements of b,
    this subset can be chosen by specifying var_selection as either a query string
    or as a list.
    """

    if var_selection is not None:
        if type(var_selection) is str:
            myb = b.query(var_selection)
        elif type(var_selection) is list:
            myb = b.loc[var_selection]
        else:
            raise(ValueError,"var_selection should be a query string of list of variable names")
    else:
        myb = b


    # Drop parts of matrix not involved in test
    myV = V.reindex(myb.index,axis=0).reindex(myb.index,axis=1)

    myV = utils.cov_nearest(myV,threshold=1e-10)

    if R is not None:
        myV = R.T@myV@R
        myb = R.T@b
        if np.isscalar(myV):
            myV = np.array([[myV]])
            myb = np.array([[myb]])

    if TEST: # Generate values of my that satisfy Var(myb)=Vb and Emyb=0
        myb = myb*0 + stats.multivariate_normal(cov=((1e0)*np.eye(myV.shape[0]) + myV)).rvs().reshape((-1,1))

    # "Invert"...

    L = np.linalg.cholesky(myV)
    y = np.linalg.solve(L.T,myb)

    chi2 = y.T@y

    y = pd.Series(y.squeeze(),index=myb.index)

    return chi2,1-stats.distributions.chi2.cdf(chi2,df=len(myb))


#+end_src

*** Test of chi2_test
#+begin_src python :tangle metrics_miscellany/test/test_chi2_test.py
import pandas as pd
from scipy import stats
from metrics_miscellany import tests
import numpy as np

def main():

    labels = ['a','b']
    D = pd.DataFrame([[2,1],[2,2]],index=labels,columns=labels)
    D.index.name = 'Variable'
    D.columns.name = 'Variable'

    V = D.T@D

    b = pd.DataFrame(stats.multivariate_normal(cov=V).rvs(),index=labels)
    b.index.name = 'Variable'

    return tests.chi2_test(b,V,"Variable in ['a']")

def test_chi2():
    p = []
    m = 1000
    for i in range(m):
        p.append(main()[1])

    p = pd.Series([x[0][0] for x in p]).squeeze()

    X = np.linspace(.05,.95,10)
    assert np.linalg.norm(p.quantile(X) - X)/len(X) < 1e-1

if __name__ == '__main__':
    test_chi2()
 #+end_src
** Skillings-Mack Test (Generalization of Friedman Test)
This implements a version of the test proposed in [cite/t:@skillings-mack81], which generalizes the Friedman rank test to the case in which data is incomplete.  Because the Friedman test is a special case, we also create a =friedman= test.
#+begin_src python :tangle metrics_miscellany/tests.py
def skillings_mack(df,bootstrap=False):
    """
    Non-parametric test of correlation across columns of df.

    Algorithm from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2761045/
    """
    def construct_statistic(R,kay):
        """
        Once we have ranks, construct SM statistic
        """
        # Fill missing ranks with (k_i+1)/2
        R = R.where(~np.isnan(R),(kay+1)/2,axis=1)

        # Construct adjusted observation matrix
        A = R.subtract((kay.values+1)/2,axis=1)@np.sqrt(12/(kay.values+1))

        # Count of observations in both columns k and l
        O = ~np.isnan(X)+0.

        Sigma = np.eye(O.shape[0]) - O@O.T

        # Delete diagonal
        Sigma = Sigma - np.diag(np.diag(Sigma))

        # Add minus column sums to diagonal
        Sigma = Sigma - np.diag(Sigma.sum())

        return A.T@np.linalg.pinv(Sigma)@A

    # Drop any rows with only one column
    X = df.loc[df.count(axis=1)>0]

    n,k = X.shape

    # Counts of obs per row ("treatments")
    kay = X.count(axis=0)

    # Counts of obs per column ("blocks")
    en = X.count(axis=1)

    R = X.rank(axis=0)

    SM = construct_statistic(R,kay)

    if not bootstrap:
        p = 1-stats.distributions.chi2.cdf(SM,df=n-1)
    else:
        if bootstrap == True:
            tol = 1e-03
        else:
            tol = bootstrap

        SE = 0
        lastSE = np.inf
        its = 0
        sms = []
        while (its < 30) or (np.abs(SE-lastSE) > tol):
            lastSE = SE
            scrambled = pd.DataFrame(np.apply_along_axis(np.random.permutation,axis=0,arr=R.values),
                                     index=R.index,columns=R.columns)
           
            sms.append(construct_statistic(scrambled,kay))
            SE = np.std(sms)
            its += 1
        p = np.mean(sms>SM)

    return SM,p

friedman = skillings_mack
#+end_src
*** Test of Skillings Mack
#+begin_src python :tangle metrics_miscellany/test/test_skillings_mack.py
import pandas as pd
import numpy as np
from metrics_miscellany import tests

def test_sm_against_R():
    """This is an example given in https://cran.r-project.org/web/packages/Skillings.Mack/Skillings.Mack.pdf
    """
    X = pd.DataFrame([[3.2, 3.1, 4.3, 3.5, 3.6, 4.5, np.nan, 4.3, 3.5],
                      [4.1, 3.9, 3.5, 3.6, 4.2, 4.7, 4.2, 4.6, np.nan],
                      [3.8, 3.4, 4.6, 3.9, 3.7, 3.7, 3.4, 4.4, 3.7],
                      [4.2, 4.,  4.8, 4., 3.9, np.nan, np.nan, 4.9, 3.9]])

    # This value of SM statistic matches that from R Skill.Mack routine
    assert np.allclose(tests.skillings_mack(X)[0],15.493)

def test_sm_type1():
    # Now try a random matrix
    ps = pd.Series([tests.skillings_mack(pd.DataFrame(np.random.rand(100,10)),bootstrap=0.02)[1] for i in range(100)])

    # p values should be distributed uniformly, with mean of 1/2
    tstat = (ps.mean()-1/2)/ps.std()

    assert np.abs(tstat)<2

if __name__=='__main__':
    test_sm_against_R()
    test_sm_type1()
#+end_src
** Randomization Inference
   Suppose we want to estimate a linear regression
   \[
       y = \alpha + X\beta + W\gamma + u.
   \]

   We obtain estimates $(b,V_b)$ of the coefficients $\beta$ and
   corresponding covariance matrix.  We want to be able to conduct a
   test of the hypothesis $R'\beta=0$.

   The idea here is to use resampling of just the variables $X$
   without replacement as a way of drawing inferences regarding
   \beta.  In particular, we randomly permute the rows of $X$,
   creating a new variable $P$, and estimate
   \[
       y = \alpha + P\delta + W\gamma + u,
   \]
   yielding estimates $(d,V_d)$ for the coefficients $\delta$ and the
   covariance matrix of these estimates.

   Note that $R'\E d = 0$ by construction, for any set of linear
   restructions $R$.  The linear restrictions themselves suggest a
   $\chi^2$ test; denote this statistic by $T(R,d,V)$.  We repeat the
   permute-estimate-test cycle many times.  Then the proportion of
   times that the test statistic associated with the test of
   $$R'(\beta-\hat\delta)>0$ gives us a \(p\)-value associated with a
   test of the null hypothesis that $\beta>c$.  A two-sided test can
   be constructed from the absolute difference in absolute values;
   i.e., $|\beta - \delta|>c$.

#+begin_src python :tangle metrics_miscellany/tests.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import ols

def randomization_inference(vars,X,y,permute_levels=None,R=None,tol=1e-3,VERBOSE=False,return_draws=False):
    """
    Return p-values associated with hypothesis that coefficients
    associated with vars are jointly equal to zero.

    Ethan Ligon                                       June 2021
    """

    assert np.all([v in X.columns for v in vars]), "vars must correspond to columns of X."

    b,V = ols(X,y)

    beta = b.squeeze()[vars]
    chi2 = chi2_test(beta,V,R=R)[0]

    last = np.inf
    p = 0
    i = 0
    Chi2 = []
    while (np.linalg.norm(p-last)>tol) or (i < 30):
        last = p
        if permute_levels is None:
            P= pd.DataFrame(np.random.permutation(X.loc[:,vars]),index=X.index,columns=vars)
        else:
            levels = X.index.names
            fixed = X.index.names.difference(permute_levels)
            P = pd.DataFrame(X.loc[:,vars].unstack(fixed).sample(frac=1).stack(fixed).values,index=X.index,columns=vars)

        myX = pd.concat([X.loc[:,X.columns.difference(vars)],P],axis=1)
        b,V = ols(myX,y)
        Chi2.append(chi2_test(b.squeeze()[vars],V,R=R)[0])
        p = (chi2<Chi2[-1])/(i+1) + last*i/(i+1)
        i += 1
        if VERBOSE: print("Latest chi2 (randomized,actual,p): (%6.2f,%6.2f,%6.4f)" % (Chi2[-1],chi2,p))

    if return_draws:
        return p,pd.Series(Chi2)
    else:
        return p

#+end_src

*** Test of randomization inference
#+begin_src ipython :tangle metrics_miscellany/test/test_randomization_inference.py
import pandas as pd
import scipy.stats.distributions as dists
from metrics_miscellany import estimators, tests
import matplotlib.pyplot as plt

n=1000
p = 0.5
# Generate contextual variables; probability of being female is p
C = pd.DataFrame({'Female':dists.binom(1,p).rvs(size=n)})
C['Male'] = 1-C

delta = pd.Series({"Female":1.,"Male":0.5})

T1 = pd.Series(dists.norm.rvs(size=n),name='Treatment1')
T2 = pd.Series(dists.norm.rvs(size=n),name='Treatment2')

# Interactions:
TC = C.multiply(T1,axis=0)
TC.columns = ['TxFemale','TxMale']

# Construct RHS matrix
X = pd.concat([T1,T2,C,TC],axis=1).iloc[:,:-1]
dC = C@delta

# Generate outcome y with *no* treatment effect, to look for Type I errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = dC + epsilon
Y.name = 'outcome'

p_i = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Generate outcome y with uniform treatment effect, to look for Type II errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = T1 + T2 + dC + epsilon
Y.name = 'outcome'

p_ii = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Test Treatment1 == Treatment2
R = pd.DataFrame({'Coefficients':[1,-1]},index=['Treatment1','Treatment2'])

p_iii = tests.randomization_inference(['Treatment1','Treatment2'],X.drop('TxFemale',axis=1),Y,R=R,VERBOSE=True)

#+end_src

** Maunchy test of sphericity
This test asks whether, given a sample covariance matrix $S$, one can
reject the hypothesis that the population covariance matrix
$\Sigma=\sigma I$; i.e., whether the random vector with variance
matrix $\Sigma$ has a spherical distribution or not (note that the
test is obtained under the assumption that the random vectors are
normally distributed), and is due to Maunchy (1940)[fn:: See
cite:muirhead82 p. 334.].
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def maunchy(C,N):
    """Given a sample covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix is proportional to the identity matrix.
    """

    raise NotImplementedError

    m = C.shape[0]

    V = np.linalg.det(C)/((np.trace(C)/m)**m)

    rho = 1 - (2*m**2 + m + 2)/(6*m*(N-1))

    w2 = (m-1)*(m-2)*(m+2)*(2*m**3 + 6*m**2 + 3*m + 2)/(288*(m**2) * ((N-1)**2) * rho**2)

    gamma = (((N-1)*rho)**2)*w2

    x2 = -2*(N-1)*rho*np.log(V)  # Chi-squared statistic

    df = (m+2)*(m-1)/2

    px2 = chi2.cdf(x2,df)

    p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** COMMENT Test of Maunchy
#+begin_src python :tangle metrics_miscellany/test/test_maunchy.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
import matplotlib.pyplot as plt

N = 300
k = 10

Chi2 = []
P = []
for m in range(1000):
    X = iid.norm.rvs(size=(N,k))

    C = np.cov(X,rowvar=False)
    x,p = tests.maunchy(C,N)
    Chi2.append(x)
    P.append(p)

df = (k+2)*(k-1)/2


range = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.plot(range,[iid.chi2.pdf(x,df) for x in range])[0]
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
#print(
#+end_src
** Inference on eigenvalues
Suppose we wish to test whether a covariance matrix has a structure $\Sigma =
\Lambda\Lambda^\T + \lambda I$, where $\Lambda$ is rank $r$.  This
structure is often assumed in exact factor models, for example.
cite:srivastava-khastri79 (\S 9.5) suggest a simple likelihood ratio test,
implemented here.
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def kr79(C,q,N):
    """Given a sample mxm covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix has last q eigenvalues equal or not, where q+k=m.
    """

    m = C.shape[0]

    l = np.linalg.eigvalsh(C)  # eigenvalues in *ascending* order

    Q = (np.prod(l[:q])/(np.mean(l[:q])**q))**(N/2) # LR test statistic

    x2 = -2*np.log(Q)  # Chi-squared statistic

    df = (q-1)*(q+2)/2

    px2 = chi2.cdf(x2,df)

    #p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** Test of kr79
#+begin_src python :tangle metrics_miscellany/test/test_kr79.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
from scipy import stats
import matplotlib.pyplot as plt

N = 3000
m = 10
r = 3
q = m - r

# Build covariance matrix for "systematic" variation
Sigma = iid.norm.rvs(size=(m,r))
Sigma = Sigma@Sigma.T   # Positive definite

l,v = np.linalg.eigh(Sigma)
l = np.maximum(l,0)

Ssqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Sigma,Ssqrt@Ssqrt.T)

# Build covariance matrix for errors.
## Use (something proportional to) identity for tests of size, otherwise tests of power.
#Psi = 10.0*np.eye(m)  + np.diag(range(1,m+1))/(m**3)
Psi = np.eye(m)
Psi = Psi@Psi.T   # Positive definite

l,v = np.linalg.eigh(Psi)
l = np.maximum(l,0) 

Psisqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Psi,Psisqrt@Psisqrt.T)


Chi2 = []
P = []
for s in range(1000):
    X = iid.norm.rvs(size=(N,m))@Ssqrt.T
    e = iid.norm.rvs(size=(N,m))@Psisqrt.T

    C = np.cov(X + e,rowvar=False)
    x,p = tests.kr79(C,q,N)
    Chi2.append(x)
    P.append(p)

df = (q+2)*(q-1)/2

xrange = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
ax.plot(xrange,[iid.chi2.pdf(x,df) for x in xrange])

assert stats.kstest(P,stats.distributions.uniform.cdf).pvalue>0.01
#+end_src
* Utils
** Some QR Tricks
*** QR Decomposition
Begin with wrapping the numpy qr decomposition to return dataframes.
#+begin_src python  :tangle metrics_miscellany/utils.py
import numpy as np
import pandas as pd

def qr(X):
    """
    Pandas-friendly QR decomposition.
    """
    assert X.shape[0]>=X.shape[1]

    Q,R = np.linalg.qr(X)
    Q = pd.DataFrame(Q,index=X.index, columns=X.columns)
    R = pd.DataFrame(R,index=X.columns, columns=X.columns)

    return Q,R
#+end_src

*** Leverage
Now, use the fact that the leverage of different observations in $X$
are the sums of squares of rows of $Q$ in the $QR$ decomposition
#+begin_src python :tangle metrics_miscellany/utils.py

def leverage(X):
    """
    Return leverage of observations in X (the diagonals of the hat matrix).
    """

    Q = qr(X)[0]

    return (Q**2).sum(axis=1)
#+end_src

*** Hat factory

Now construct a factory that returns a function to put the "hat"
on y.  Though mathematically this looks like $X(X'X)^{-1}X'=QQ'$ in
practice we don't want to construct an $N\times N$ matrix like this,
as it's often too expensive.

#+begin_src python :tangle metrics_miscellany/utils.py
def hat_factory(X):
    """
    Return a function hat(y) that returns X(X'X)^{-1}X'y.

    This is the least squares prediction of y given X.

    We use the fact that  the hat matrix is equal to QQ',
    where Q comes from the QR decomposition of X.
    """
    Q = qr(X)[0]

    def hat(y):
        return Q@(Q.T@y)

    return hat
#+end_src


**  "Fixing" matrices that aren't quite positive definite
#+begin_src python :tangle metrics_miscellany/utils.py
from statsmodels.stats.correlation_tools import cov_nearest as _cov_nearest
import pandas as pd

def cov_nearest(V,threshold=1e-12):
    """
    Return a positive definite matrix which is "nearest" to the symmetric matrix V,
    with the smallest eigenvalue not less than threshold.
    """
    s,U = np.linalg.eigh((V+V.T)/2) # Eigenvalue decomposition of symmetric matrix

    s = np.maximum(s,threshold)

    return V*0 + U@np.diag(s)@U.T  # Trick preserves attributes of dataframe V
#+end_src
** Trimming
#+begin_src python :tangle metrics_miscellany/utils.py
import pandas as pd
import numpy as np

def trim(df,alpha):
    """Trim values below alpha quantile and above (1-alpha) quantile.

    This maps individual extreme elements of df to NaN.
    """
    xmin = df.quantile(alpha)
    xmax = df.quantile(1-alpha)
    return df.where((df>=xmin)*(df<=xmax),np.nan)
#+end_src

* Random
** Random permutations of dataframes
#+begin_src python :tangle metrics_miscellany/random.py
import numpy as np
import pandas as pd

def permutation(df,columns=None,permute_levels=None):
    """Randomly permute rows of df[columns] at permute_levels.
    """

    df = pd.DataFrame(df) # Make sure we have a DataFrame.

    if columns is None: columns = df.columns

    if permute_levels is None:
        P = pd.DataFrame(np.random.permutation(df.loc[:,columns]),index=df.index,columns=columns)
    else:
       fixed = df.index.names.difference(permute_levels)
       P = pd.DataFrame(df.loc[:,columns].unstack(fixed).sample(frac=1).stack(fixed).values,index=df.index,columns=columns)

    return P

#+end_src
*** Test permutation
#+begin_src python :tangle metrics_miscellany/test/test_permutation.py
import numpy as np
import pandas as pd
from metrics_miscellany import random

T = pd.Series(np.random.rand(10)>.5)

df = pd.DataFrame({'a':T,'b':T}).stack()
df = df + 0
df.index.names = ['i','t']

p = random.permutation(df,permute_levels=['i'])

assert np.all(p.unstack('t').std(axis=1)==0)

#+end_src
