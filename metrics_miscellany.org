* DataFrame Utils
Here we provide various utils meant to operate on pandas DataFrames and Series.  Note that because datamat classes inherit from pandas these functions can be used with the datamat classes as well.
** Unary Matrix Operators
#+begin_src python :tangle metrics_miscellany/utils.py
import numpy as np
from scipy import sparse as scipy_sparse
import pandas as pd

def inv(A):
    """Inverse of square pandas DataFrame."""
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.inv(A)
    return pd.DataFrame(B,columns=A.columns,index=A.index)

def pinv(A):
    """Moore-Penrose pseudo-inverse of A.

    >>> A = pd.DataFrame([[1,2,3],[4,5,6]])
    >>> np.allclose(A@pinv(A),np.eye(2))
    True
    """
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.pinv(A)
    return pd.DataFrame(B,columns=A.index,index=A.columns)

def leverage(X):
    """
    Leverage of matrix X; i.e., diagonal of projection matrix.
    """
    return (X*pinv(X).T).sum(axis=1)

def svd(A,hermitian=False):
    """Singular value composition into U@S.dg@V.T."""
    idx = A.index
    cols = A.columns
    u,s,vt = np.linalg.svd(A,compute_uv=True,full_matrices=False,hermitian=hermitian)
    u = pd.DataFrame(u,index=idx)
    vt = pd.DataFrame(vt,columns=cols)
    s = pd.Series(s)

    return u,s,vt

def eig(A,hermitian=False):
    """Singular value composition into U@S.dg@V.T."""
    idx = A.index
    cols = A.columns
    if hermitian:
        s2,u = np.linalg.eigh(A)
    else:
        s2,u = np.linalg.eig(A)

    s2 = np.flip(s2)
    u = np.fliplr(u)

    u = pd.DataFrame(u,index=idx,columns=cols)
    s2 = pd.Series(s2.squeeze())

    return s2,u

def diag(X,sparse=True):

    try:
        assert X.shape[0] == X.shape[1]
        d = pd.Series(np.diag(X),index=X.index)
    except IndexError: # X is a series?
        if sparse:
            # We can wind up blowing ram if not careful...
            d = scipy_sparse.diags(X.values)
            d = pd.DataFrame.sparse.from_spmatrix(d,index=X.index,columns=X.index)
        else:
            raise NotImplementedError
    except AttributeError: # Not a pandas object?
        d = np.diag(X)

    return d



#+end_src

** Binary Matrix Products
#+begin_src python :tangle metrics_miscellany/utils.py
def outer(S,T):
    """Outer product of two series (vectors) S & T.
    """
    return pd.DataFrame(np.outer(S,T),index=S.index,columns=T.index)

def matrix_product(X,Y,strict=False,fillmiss=True):
    """Compute matrix product X@Y, allowing for possibility of missing data.

    The "strict" flag if set requires that the names of levels of indices that vary for columns of X be in the intersection of names of levels of indices that vary for rows of Y.
    """

    if strict and not all(X.columns==Y.index):  # Columns and Indices don't match.
        X.columns = drop_vestigial_levels(X.columns)
        Y.index = drop_vestigial_levels(Y.index)

    if fillmiss:
        X = X.fillna(0)
        Y = Y.fillna(0)

    prod = np.dot(X,Y) #.squeeze()

    if len(prod.shape)==1 or prod.shape[1]==1:
        out = pd.Series(prod.squeeze(),index=X.index)
    else:
        try:
            cols = Y.columns
        except AttributeError:
            cols = None
        out = pd.DataFrame(prod,index=X.index,columns=cols)

    return out

def self_inner(X,min_obs=None):
    """Compute inner product X.T@X, allowing for possibility of missing data."""
    n,m=X.shape

    if n<m:
        axis=1
        N=m
    else:
        axis=0
        N=n

    xbar=X.mean(axis=axis)

    if axis:
        C=(N-1)*X.T.cov(min_periods=min_obs)
    else:
        C=(N-1)*X.cov(min_periods=min_obs)

    return C + N*np.outer(xbar,xbar)

def kron(A,B,sparse=False):
    if sparse:
        from scipy.sparse import kron

    if isinstance(A,pd.DataFrame):
        a = A.values
        if isinstance(B,pd.DataFrame):
            columns = pd.MultiIndex.from_tuples([(*i,*j) for i in A.columns for j in B.columns],                                               names=A.columns.names+B.columns.names)
            b = B.values
        else:
            columns = A.columns.remove_unused_levels()
            b = B.values.reshape((-1,1))
    elif isinstance(B,pd.DataFrame):
        columns = B.columns.remove_unused_levels()
        a = A.values.reshape((-1,1))
        b = B.values

    index = pd.MultiIndex.from_tuples([(*i,*j) for i in A.index for j in B.index],
                                      names=A.index.names+B.index.names)

    if sparse:
        a = kron(a,b)
        return pd.DataFrame.sparse.from_spmatrix(a,columns=columns,index=index)
    else:
        a = np.kron(a,b)
        return pd.DataFrame(a,columns=columns,index=index)

#+end_src
*** Binary Operation Tests
#+begin_src python :tangle metrics_miscellany/test/test_index_multiplication.py
import pytest
import datamat as dm
import pandas as pd
import numpy as np

@pytest.fixture
def setup_indices():
    idx = pd.MultiIndex.from_tuples([(0,0,0),(0,0,1),(1,0,0),(1,0,1)], names=['i','j','k'])
    return idx

@pytest.fixture
def setup_data_matrices(setup_indices):
    X = dm.DataMat([[1,2,3,4]], columns=setup_indices, idxnames=['l'])
    Y = dm.DataMat([[1,2,3,0]], columns=setup_indices.droplevel('j'), idxnames='m').T
    return X, Y

def test_index_multiplication(setup_data_matrices):
    X, Y = setup_data_matrices
    result = X @ Y
    assert result.index.names == ['l']
    X.matmul(Y, strict=True)

if __name__=='__main__':
    pytest.main()
#+end_src

#+begin_src python :tangle metrics_miscellany/test/test_binary_ops.py
import pytest
import datamat as dm
import pandas as pd
import numpy as np

@pytest.fixture
def numpy_matrices():
    A = np.array([[1,2],[3,4]])
    B = np.array([[1,1]]).T
    return A, B

@pytest.fixture
def pandas_matrices():
    A = pd.DataFrame([[1,2],[3,4]])
    B = pd.DataFrame([[1,1]]).T
    return A, B

@pytest.fixture
def datamat_matrices():
    A = dm.DataMat([[1,2],[3,4]])
    B = dm.DataMat([[1,1]]).T
    return A, B

def test_matmul(numpy_matrices, pandas_matrices, datamat_matrices):
    for A, B in [numpy_matrices, pandas_matrices, datamat_matrices]:
        C = A @ B
        if isinstance(A, dm.DataMat):
            expected = dm.DataVec if getattr(B, "shape", (None, None))[1] == 1 else dm.DataMat
            assert isinstance(C, expected)
        else:
            assert isinstance(C, type(A))

@pytest.fixture
def datamat_vector():
    A = dm.DataMat([[1,2],[3,4]])
    b = dm.DataVec([1,1])
    return A, b

@pytest.fixture
def pandas_vector():
    A = dm.DataMat([[1,2],[3,4]])
    b = pd.Series([1,1])
    return A, b

def test_matmul_matvec(datamat_vector, pandas_vector):
    for A, b in [datamat_vector, pandas_vector]:
        C = A @ b
        assert isinstance(C, type(b))

if __name__=='__main__':
    pytest.main()
#+end_src

** Matrix Decompositions
#+begin_src python :tangle metrics_miscellany/utils.py
def heteropca(C,r=1,max_its=50,tol=1e-3,verbose=False):
    """Estimate r factors and factor weights of covariance matrix C."""
    from scipy.spatial import procrustes

    N = C - np.diag(np.diag(C))

    ulast = np.zeros((N.shape[1],r))
    u = np.zeros((N.shape[1],r))
    u[0,0] = 1
    ulast[-1,0] = 1

    t = 0

    while procrustes(u,ulast)[-1] >tol and t<max_its:
        ulast = u

        u,s,vt = np.linalg.svd(N,full_matrices=False,hermitian=True)

        s = s[:r]
        u = u[:,:r]

        Ntilde = u[:,:r]@np.diag(s[:r])@vt[:r,:]

        N = N - np.diag(np.diag(N)) + np.diag(np.diag(Ntilde))

        t += 1

        if t==max_its:
            warnings.warn("Exceeded maximum iterations (%d)" % max_its)
        if verbose: print(f"Iteration {t}, u[0,:r]={u[0,:r]}.")

    return u,s

def svd_missing(A,max_rank=None,min_obs=None,heteroskedastic=False,verbose=False):
    """Singular Value Decomposition with missing values

    Returns matrices U,S,V.T, where A~=U*S*V.T.

    Inputs:
        - A :: matrix or pd.DataFrame, with NaNs for missing data.

        - max_rank :: Truncates the rank of the representation.  Note
                      that this impacts which rows of V will be
                      computed; each row must have at least max_rank
                      non-missing values.  If not supplied rank may be
                      truncated using the Kaiser criterion.

        - min_obs :: Smallest number of non-missing observations for a
                     row of U to be computed.

        - heteroskedastic :: If true, use the "heteroPCA" algorithm
                       developed by Zhang-Cai-Wu (2018) which offers a
                       correction to the svd in the case of
                       heteroskedastic errors.  If supplied as a pair,
                       heteroskedastic[0] gives a maximum number of
                       iterations, while heteroskedastic[1] gives a
                       tolerance for convergence of the algorithm.

    Ethan Ligon                                        September 2021

    """
    # Defaults; modify by passing a tuple to heteroskedastic argument.
    max_its=50
    tol = 1e-3

    P = self_inner(A,min_obs=min_obs) # P = A.T@A

    sigmas,v=np.linalg.eigh(P)

    order=np.argsort(-sigmas)
    sigmas=sigmas[order]

    # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
    v=v[:,order]
    v=v[:,sigmas>0]
    s=np.sqrt(sigmas[sigmas>0])

    if max_rank is not None and len(s) > max_rank:
        v=v[:,:max_rank]
        s=s[:max_rank]

    r=len(s)

    if heteroskedastic: # Interpret tuple
        try:
            max_its,tol = heteroskedastic
        except TypeError:
            pass
        Pbar = P.mean()
        v,s = heteropca(P-Pbar,r=r,max_its=max_its,tol=tol,verbose=verbose)

    if A.shape[0]==A.shape[1]: # Symmetric; v=u
        return v,s,v.T
    else:
        vs=v@np.diag(s)

        u=np.zeros((A.shape[0],len(s)))
        for j in range(A.shape[0]):
            a=A.iloc[j,:].values.reshape((-1,1))
            x=np.nonzero(~np.isnan(a))[0] # non-missing elements of vector a
            if len(x)>=r:
                u[j,:]=(np.linalg.pinv(vs[x,:])@a[x]).reshape(-1)
            else:
                u[j,:]=np.nan

    s = pd.Series(s)
    u = pd.DataFrame(u,index=A.index)
    v = pd.DataFrame(v,index=A.columns)

    return u,s,v

def sqrtm(A,hermitian=False):
    """
    Return a positive semi-definite square root for the matrix A.

    NB: A must itself be positive semi-definite.
    """
    u,s,vt = svd(A,hermitian=hermitian)

    if np.any(s<0):
        raise ValueError("Matrix must be positive semi-definite.")

    return u@np.diag(np.sqrt(s))@vt

def cholesky(A):
    """
    Cholesky decomposition A = L@L.T; return lower-triangular L.
    """
    L = np.linalg.cholesky(A)
    return pd.DataFrame(L,index=A.index,columns=A.columns)
#+end_src
** DataFrame/Mat Manipulations
#+begin_src python :tangle metrics_miscellany/utils.py
from pandas import concat, get_dummies, MultiIndex

def drop_missing(X,infinities=False):
    """
    Return tuple of pd.DataFrames in X with any
    missing observations dropped.  Assumes common index.

    If infinities is false values of plus or minus infinity are
    treated as missing values.
    """

    if isinstance(X,dict):
        return dict(zip(X.keys(),drop_missing(list(X.values()),infinities=False)))

    for i,x in enumerate(X):
        if type(x)==pd.Series and x.name is None:
            x.name = i

    foo=pd.concat(X,axis=1)
    if not infinities:
        foo.replace(np.inf,np.nan)
        foo.replace(-np.inf,np.nan)

    foo = foo.dropna(how='any')

    assert len(set(foo.columns))==len(foo.columns) # Column names must be unique!

    Y=[]
    for x in X:
        Y.append(foo.loc[:,pd.DataFrame(x).columns])

    return tuple(Y)

def dummies(df,cols,suffix=False):
    """From a dataframe df, construct an array of indicator (dummy) variables,
    with a column for every unique row df[cols]. Note that the list cols can
    include names of levels of multiindices.

    The optional argument =suffix=, if provided as a string, will append suffix
    to column names of dummy variables. If suffix=True, then the string '_d'
    will be appended.
    """
    idxcols = list(set(df.index.names).intersection(cols))
    colcols = list(set(cols).difference(idxcols))

    if len(idxcols):
        idx = use_indices(df,idxcols)
        v = concat([idx,df[colcols]],axis=1)
    else:
        v = df[colcols]

    usecols = []
    for s in idxcols+colcols:
        usecols.append(v[s].squeeze())

    tuples = pd.Series(list(zip(*usecols)),index=v.index)

    v = get_dummies(tuples).astype(int)

    if suffix==True:
        suffix = '_d'

    if suffix!=False and len(suffix)>0:
        columns = [tuple([str(c)+suffix for c in t]) for t in v.columns]
    else:
        columns = v.columns

    v.columns = MultiIndex.from_tuples(columns,names=idxcols+colcols)

    return v
#+end_src

*** Test of =dummies=
#+begin_src python :tangle metrics_miscellany/test/test_dummies.py
from metrics_miscellany.utils import dummies, use_indices
import pandas as pd

def test_dummies():
    idx = pd.MultiIndex.from_tuples([(i,) for i in range(4)],names=['i'])
    foo = pd.DataFrame({'cat':['a','b','b','c']},index=idx)

    assert dummies(foo,['i']).shape == (4,4)
    assert dummies(foo,['cat']).shape == (4,3)

if __name__=='__main__':
    test_dummies()

#+end_src

** Index utilities
#+begin_src python :tangle metrics_miscellany/utils.py
import pandas as pd

def use_indices(df,idxnames):
    if len(set(idxnames).intersection(df.index.names))==0:
        return pd.DataFrame(index=df.index)

    try:
        idx = df.index
        df = df.reset_index()[idxnames]
        df.index = idx
        return df
    except InvalidIndexError:
        return df

def drop_vestigial_levels(idx,axis=0,both=False,multiindex=True):
    """
    Drop levels that don't vary across the index.

    >>> idx = pd.MultiIndex.from_tuples([(1,1),(1,2)],names=['i','j'])
    >>> drop_vestigial_levels(idx)
    Index([1, 2], dtype='int64', name='j')
    """
    if both:
        return drop_vestigial_levels(drop_vestigial_levels(idx,axis=1))

    if axis==1:
        idx = idx.T

    if isinstance(idx,(pd.DataFrame,pd.Series)):
        df = idx
        idx = df.index
        HumptyDumpty = True
    else:
        HumptyDumpty = False

    try:
        l = 0
        L = len(idx.levels)
        while l < L:
            if len(set(idx.codes[l]))<=1:
                idx = idx.droplevel(l)
                L -= 1
            else:
                l += 1
                if l>=L: break
    except AttributeError:
        pass

    if multiindex and not isinstance(idx,pd.MultiIndex): # Return a multiindex, not an Index
        idx = pd.MultiIndex.from_tuples(idx.str.split('|').tolist(),names=[idx.name])

    if HumptyDumpty:
        df.index = idx
        idx = df
        if axis==1:
            idx = idx.T

    return idx

#+end_src
*** Test of =use_indices=
#+begin_src python :tangle metrics_miscellany/test/test_use_indices.py
import pandas as pd
from metrics_miscellany.utils import use_indices
import numpy as np

def test_use_indices():
    idx = pd.MultiIndex.from_tuples([(i,) for i in range(4)],names=['i'])
    foo = pd.DataFrame({'cat':['a','b','b','c']},index=idx)

    assert use_indices(foo,['i']).shape[0]==foo.shape[0]
    assert use_indices(foo,['cat']).size==0
    assert use_indices(foo,['cat','i']).shape[0]==foo.shape[0]

    assert np.all(use_indices(foo,['i']).index==idx)

if __name__=='__main__':
    test_use_indices()


#+end_src
* Estimators
** Preliminaries
#+begin_src python :tangle metrics_miscellany/estimators.py
import statsmodels.api as sm
from statsmodels.stats import correlation_tools
import numpy as np
from numpy.linalg import lstsq
import warnings
import pandas as pd
from . import gmm
from . GMM_class import GMM
from . import utils
from datamat import DataMat, DataVec
import datamat as dm
#+end_src
** OLS
#+begin_src python :tangle metrics_miscellany/estimators.py

def ols(X,y,cov_type='HC3',PSD_COV=False):
    """OLS estimator of b in y = Xb + u.

    Returns both estimate b as well as an estimate of Var(b).

    The estimator used for the covariance matrix depends on the
    optional argument =cov_type=.

    If optional flag PSD_COV is set, then an effort is made to ensure that
    the estimated covariance matrix is positive semi-definite.  If PSD_COV is
    set to a positive float, then this will be taken to be the smallest eigenvalue
    of the 'corrected' matrix.
    """
    n,k = X.shape

    est = sm.OLS(y,X).fit()
    b = pd.DataFrame({'Coefficients':est.params.values},index=X.columns)
    if cov_type=='HC3':
        V = est.cov_HC3
    elif cov_type=='OLS':
        XX = X.T@X
        if np.linalg.eigh(XX)[0].min()<0:
            XX = correlation_tools.cov_nearest(XX,method='nearest')
            warnings.warn("X'X not positive (semi-) definite.  Correcting!  Estimated variances should not be affected.")
        V = est.resid.var()*np.linalg.inv(XX)
    elif cov_type=='HC2':
        V = est.cov_HC2
    elif cov_type=='HC1':
        V = est.cov_HC1
    elif cov_type=='HC0':
        V = est.cov_HC0
    else:
        raise ValueError("Unknown type of covariance matrix.")

    if PSD_COV:
        if PSD_COV is True:
            PSD_COV = (b**2).min()
        s,U = np.linalg.eigh((V+V.T)/2)
        if s.min()<PSD_COV:
            oldV = V
            V = U@np.diag(np.maximum(s,PSD_COV))@U.T
            warnings.warn("Estimated covariance matrix not positive (semi-) definite.\nCorrecting! Norm of difference is %g." % np.linalg.norm(oldV-V))

    V = pd.DataFrame(V,index=X.columns,columns=X.columns)

    return b,V

#+end_src

*** OLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_ols.py
import pandas as pd
from metrics_miscellany.estimators import ols
import numpy as np

def test_ols(N=500000,tol=1e-2):

    x = pd.DataFrame({'x':np.random.standard_normal((N,))})
    x['Constant'] = 1

    beta = pd.DataFrame({'Coefficients':[1,0]},index=['x','Constant'])

    u = pd.DataFrame(np.random.standard_normal((N,))/10)

    y = (x@beta).values + u.values
    b,V = ols(x,y)

    assert np.allclose(b,beta,atol=tol)

if __name__=='__main__':
    test_ols()

#+end_src
** Two-stage Least Squares
#+begin_src python :tangle metrics_miscellany/estimators.py
def restricted_tsls(y,X,R=None,r=None,Z=None,cov='HC3'):
    """
    Estimate b in y = Xb + u s.t. Rb = r.

    Return b and lm, a vector of Lagrange multipliers associated with the constraints,
    as well as estimates of Omega = E Z'ee'Z and the covariance matrix of b.
    """
    if Z is None:
        Z = X

    N,k = X.shape
    _,l = Z.shape

    if R is not None:
        m,__ = R.shape
        assert __==k, "Matrix of restrictions must be conformable with vector of parameters."
    else:
        m = 0

    assert N==_, "X & Z must have same number of rows."
    assert l+m>=k, f"Need number of instruments ({l}) plus restrictions ({m}) greater than or equal to number of parameters ({k})."

    Qzz = Z.T@Z/N
    Qxz = X.T@Z/N

    if Qzz.shape==(1,): # Scalar
        Qzzinv = 1/Qzz
    else:
        Qzzinv = Qzz.inv

    Q = N*Qxz@(Qzzinv)@Qxz.T

    if R is not None:
        QRT = dm.concat({'beta':Q,'lm':R.T},axis=1,levelnames=True,toplevelname='parms')
        R0 = dm.concat({'beta':R,
                        'lm':dm.DataMat(index=R.index,columns=R.index).fillna(0)},
                       axis=1,levelnames=True,toplevelname='parms')
        lhs = dm.concat({'beta':QRT,'lm':R0},levelnames=True,toplevelname='parms')
        rhs = dm.concat({'beta':Qxz@(Qzzinv)@Z.T@y,'lm':r},levelnames=True,toplevelname='parms')
        XR = dm.concat([X,R.T],axis=1,levelnames=True)
        ZR = dm.concat([Z,R.T],axis=1,levelnames=True)
    else:
        lhs = Q
        rhs = Qxz@(Qzzinv)@Z.T@y
        XR = X
        ZR = Z

    #assert lhs.rank()>=lhs.shape[1], "Rank condition violated"

    B = lhs.lstsq(rhs)

    e = y - XR@B

    if cov=='HC2': # Use prediction errors
        e = e/np.sqrt((1-X.leverage))
    elif cov=='HC3':
        e = e/(1-X.leverage)

    Ze = ZR.multiply(e,axis=0)
    Omega = Ze.T@Ze/N

    # Redfine Qs to include restrictions
    Qzz = ZR.T@ZR/N
    Qxz = XR.T@ZR/N

    if Qzz.shape==(1,): # Scalar
        Qzzinv = 1/Qzz
    else:
        Qzzinv = Qzz.inv

    D = Qxz@Qzzinv@Qxz.T

    if D.shape==(1,): # Scalar
        Dinv = 1/D
    else:
        Dinv = D.inv

    V = Dinv@(Qxz@Qzzinv@Omega@Qzzinv@Qxz.T)@Dinv/N

    if cov=='HC1':
        V = V*(N/(N-k))

    if R is None:
        return B,Omega,V
    else:
        return B,lm,Omega,V
#+end_src

#+begin_src python :tangle metrics_miscellany/estimators.py
def tsls(X,y,Z,return_Omega=False,**kwargs):
    """
    Two-stage least squares estimator.
    """
    b,Omega,Vb = restricted_tsls(y,X,Z=Z,**kwargs)

    if return_Omega:
        return b,Omega
    else:
        return b,Vb

#+end_src
*** TSLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_tsls.py
import pandas as pd
from metrics_miscellany.estimators import tsls, ols, restricted_tsls
from datamat import DataMat, DataVec
import numpy as np

def test_tsls(N=500000,tol=1e-2):

    z = DataMat({'z':np.random.standard_normal((N,))})
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.squeeze() + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    b,Omega,V = restricted_tsls(y,x,Z=z)
    b_,V_ = tsls(x,y,z)
    #b,V = ols(x,y)

    assert np.allclose(b,beta.squeeze(),atol=tol)

    #return b,V,b_,V_

if __name__=='__main__':
    test_tsls()

#+end_src
** Factor Analysis (MLE)
This routine adapted from sklearn's FactorAnalysis, but modified to permit missing values.
#+begin_src python :tangle metrics_miscellany/estimators.py
def factor_analysis(X,n_components=None,noise_variance_init=None,
                    max_its=1000,tol=1e-2,
                    svd_method='eig'):
    """Fit the FactorAnalysis model to X using SVD based MLE approach.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features) Training data.

    n_components : Proposed rank (number of factors)
    """

    n_samples, n_features = X.shape

    assert n_samples >= n_features

    if n_components is None:
        n_components = n_features

    xbar = X.mean(axis=0)
    X = X - xbar

    # some constant terms
    nsqrt = np.sqrt(n_samples)
    llconst = n_features * np.log(2.0 * np.pi) + n_components
    var = X.var()

    if noise_variance_init is None:
        psi = np.ones(n_features)
    else:
        if len(noise_variance_init) != n_features:
            raise ValueError(
                    "noise_variance_init dimension does not accord "
                    "with number of features : %d != %d"
                    % (len(noise_variance_init), n_features)
            )
        psi = np.array(noise_variance_init)

    loglike = []
    old_ll = -np.inf
    SMALL = 1e-12

    def squared_norm(x):
        return np.linalg.norm(x)**2

    def self_inner(X,min_obs=None):
        """Compute inner product X.T@X, allowing for possibility of missing data."""
        n,m=X.shape

        if n<m:
            axis=1
            N=m
        else:
            axis=0
            N=n

        mX = np.ma.masked_invalid(X)

        xbar = np.mean(mX,axis=axis)

        if axis:
            C=(N-1)*np.ma.cov(mX)
        else:
            C=(N-1)*np.ma.cov(mX.T)

        return (C + N*np.outer(xbar,xbar)).data

    # we'll modify svd outputs to return unexplained variance
    # to allow for unified computation of loglikelihood
    if svd_method == "lapack":

        def my_svd(X):
            _, s, Vt = linalg.svd(X, full_matrices=False, check_finite=False)
            return (
                s[:n_components],
                Vt[:n_components],
                squared_norm(s[n_components:]),
            )

    elif svd_method == "randomized":
        random_state = check_random_state(self.random_state)

        def my_svd(X):
            _, s, Vt = randomized_svd(
                X,
                n_components,
                random_state=random_state,
                n_iter=self.iterated_power,
            )
            return s, Vt, squared_norm(X) - squared_norm(s)

    elif svd_method == 'eig':

        def my_svd(P):

            sigmas,v=np.linalg.eigh(P)
            vt = v.T

            order=np.argsort(-sigmas)
            sigmas=sigmas[order]

            # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
            vt=vt[order,:]
            vt=vt[sigmas>0,:]
            s=np.sqrt(sigmas[sigmas>0])

            if n_components is not None and len(s) > n_components:
                vt=vt[:n_components,:]
                s=s[:n_components]

            r=len(s)

            return s, vt, squared_norm(P) - squared_norm(s)

    P = self_inner(X)
    for i in range(max_its):
        # SMALL helps numerics
        sqrt_psi = np.sqrt(psi) + SMALL
        s, Vt, unexp_var = my_svd(P@np.diag(1/(psi * n_samples)))
        s **= 2
        # Use 'maximum' here to avoid sqrt problems.
        W = np.sqrt(np.maximum(s - 1.0, 0.0))[:, np.newaxis] * Vt
        del Vt
        W = W.squeeze()*sqrt_psi

        # loglikelihood
        ll = llconst + np.sum(np.log(s))
        ll += unexp_var + np.sum(np.log(psi))
        ll *= -n_samples / 2.0
        loglike.append(ll)
        if (ll - old_ll) < tol:
            break
        old_ll = ll

        psi = np.maximum(var - np.sum(W**2, axis=0), SMALL)
    else:
        if max_its==0: # Use hill-climbing
            def ll():
                ll = llconst + np.sum(np.log(s))
                ll += unexp_var + np.sum(np.log(psi))
                ll *= -n_samples / 2.0

        else:
            warnings.warn(
                "FactorAnalysis did not converge."
                + " You might want"
                + " to increase the number of iterations.",
                ConvergenceWarning,
            )

    return W, psi

#+end_src


#+begin_src python
from scipy.optimize import minimize

def factor_analysis(V,r=1,psi=None,method='mle',tol=1e-12):

    p = V.shape[0]

    def factor_loadings(Psi):
        U = V - Psi.dg()
        s2,u = U.eig()
        s2 = s2.iloc[-r:]
        u = u.iloc[:,-r:]

        W = np.sqrt(s2.dg())@u.T

        return W

    def ll(psi):
        Psi = dm.DataMat(np.diag(psi**2))
        W = factor_loadings(Psi)

        Vhat = W.T@W + Psi

        return -(p/2)*np.log(2*np.pi) - np.log(Vhat.det)/2 - (Vhat.inv@V).trace/2

    if psi is None or np.linalg.norm(psi)<tol:
        psi = np.ones(p)

    if method=='mle':
        out = minimize(lambda x: -ll(x),x0=psi)
        if not out.success:
            print(out)

        Psi = dm.DataMat(np.diag(out.x**2))
    else:  # Iterative approach
        Psi = dm.DataMat(np.diag(psi**2))
        oldPsi = 0
        its = 0
        while (Psi-oldPsi).norm()>1e-8:
            its += 1
            print(its)
            W = factor_loadings(Psi)
            oldPsi = Psi
            Psi = (V - W.T@W).dg().dg()


    W = factor_loadings(Psi)

    return W,Psi
#+end_src
** Frisch-Waugh-Lovell Regression
#+begin_src python :tangle metrics_miscellany/estimators.py
def fwl_regression_step(D,X):
    """Regress each datamat in dictionary D on X.
       Return a dictionary of residuals, and a dictionary of least-squares coefficients.
    """
    b = {}
    u = {}
    if len(D)==0: return D,{}

    for k,v in D.items():
        b[k] = X.lstsq(v)
        u[k] = dm.DataMat(v.resid(X))

    return u,b
#+end_src

#+begin_src python :tangle metrics_miscellany/estimators.py
def fwl_regression(D,B=None,U=None):
    """Regress each datamat in dictionary D on the last element X of D.
       Iterate.

       Return a dictionary of residuals, and a dictionary of least-squares coefficients.
    """
    if B is None: B={}
    if U is None: U={}

    if len(D)==0:
        return {},{}
    elif len(D)==1:
        return U,B
    else:
        xk,x = D.popitem()
        D,B[xk] = fwl_regression_step(D,x)
        U[xk] = D.copy()
        return fwl_regression(D,B=B,U=U)
#+end_src

#+begin_src python :tangle metrics_miscellany/estimators.py
def reconstruct_coefficients_from_fwl(B: dict,as_dict=False):
    """
    Reconstructs OLS coefficient vectors from FWL inputs,
    generalized for matrix regressors.
    """
    # ## 1. Infer the dependent variable name ##
    top_level_keys = set(B.keys())
    if len(top_level_keys)==0: return {}

    # Arbitrarily pick the first variable's sub-dictionary to inspect its keys
    first_var_key = next(iter(B))
    all_nested_keys = set(B[first_var_key].keys())

    # The dependent variable is the key in the nested dict that is NOT a top-level key
    dep_var_set = all_nested_keys - top_level_keys
    if len(dep_var_set) != 1:
        raise ValueError("Could not uniquely determine the dependent variable name.")
    dep_var_name = dep_var_set.pop()

    # ## 2. Proceed with the iterative reconstruction ##
    ordered_vars = list(B.keys())
    p = len(ordered_vars)
    coeffs: dict[str, pd.Series] = {}  # Stores the final coefficient vectors (b_i)

    # This loop proceeds backward from i = p-1 down to 0
    for i in range(p - 1, -1, -1):
        current_var = ordered_vars[i]

        # Use the inferred dependent variable name to get the G_iy vector
        G_iy = B[current_var][dep_var_name]

        summation_vector = np.zeros_like(G_iy)

        for j in range(i + 1, p):
            successor_var = ordered_vars[j]
            G_ij = B[current_var][successor_var]
            b_j = coeffs[successor_var]
            summation_vector += G_ij @ b_j

        coeffs[current_var] = G_iy - summation_vector
        coeffs[current_var].name = dep_var_name

    # Reverse order of dict
    coeffs = {k:coeffs[k] for k in reversed(list(coeffs.keys()))}

    if as_dict:
        return coeffs
    else:
        return dm.concat(coeffs,levelnames=True).squeeze()

#+end_src


*** Unit tests for =fwl_regression= and =reconstruct_coefficients_from_fwl=.

Here are unit tests for both functions:

#+begin_src python :tangle metrics_miscellany/test/test_fwl.py
import unittest
import numpy as np
import pandas as pd
from metrics_miscellany.estimators import fwl_regression, reconstruct_coefficients_from_fwl
import datamat as dm

class TestFWLRegression(unittest.TestCase):
    def setUp(self):
        # Create sample data for testing
        np.random.seed(42)

        N = 1000

        D = {}
        D['Constant'] = dm.DataMat(np.ones(N))
        D['a'] = dm.DataMat(np.random.randn(N,2))
        D['b'] = D['a']*2 + np.random.randn(N,2)
        D['y'] = dm.DataMat(D['a']@np.array((1,1)) + + D['b']@np.array((2,2)) + np.random.randn(N,)/1000,name='y')

        # Reverse order of dict
        D = dict([D.popitem() for i in range(len(D))])

        self.D = D

        # Direct OLS for comparison
        YX = dm.concat(D,levelnames=True,axis=1)
        Y = YX.xs('y',level='v',axis=1,drop_level=False)
        X = YX.iloc[:,1:]

        self.direct_coefs = X.lstsq(Y)

    def test_fwl_regression(self):
        # Run FWL regression
        U, B = fwl_regression(self.D.copy())

        # Check if U & B contains entries for all variables
        self.assertEqual(set(B.keys()), set(U.keys()))

        # Test that the last stage regression gives correct coefficients
        # The last variable (X3) should have the coefficient of Y on X3 after controlling for X1, X2
        # Reconstruct coefficients
        reconstructed = reconstruct_coefficients_from_fwl(B,as_dict=False)

        np.testing.assert_allclose(reconstructed-self.direct_coefs,0,atol=1e-2)

    def test_reconstruct_coefficients(self):
        # Run FWL regression
        U, B = fwl_regression(self.D.copy())

        # Reconstruct coefficients
        reconstructed = reconstruct_coefficients_from_fwl(B,as_dict=True)

        # Check if reconstructed coefficients match direct OLS
        for i, var in enumerate(['X3', 'X2', 'X1']):
            if var in reconstructed:
                self.assertAlmostEqual(reconstructed[var], self.direct_coefs[i], places=2)

    def test_empty_dict(self):
        # Test with empty dictionary
        U, B = fwl_regression({})
        self.assertEqual(U, {})
        self.assertEqual(B, {})

        # Test reconstruction with empty results
        reconstructed = reconstruct_coefficients_from_fwl({})
        self.assertEqual(reconstructed, {})

    def test_single_variable(self):
        # Test with a single variable
        D_single = {'y': self.D['y']}
        U, B = fwl_regression(D_single)
        self.assertEqual(U, {})
        self.assertEqual(B, {})

if __name__ == '__main__':
    foo = TestFWLRegression()
    foo.setUp()
    #foo.test_fwl_regression()
    unittest.main()
#+end_src



*** Test
#+begin_src python :tangle metrics_miscellany/test/test_fwl_regression.py
import datamat as dm
from metrics_miscellany.estimators import fwl_regression
import numpy as np

N = 100

D = {}
D['Constant'] = dm.DataMat(np.ones(N))
D['a'] = dm.DataMat(np.random.randn(N,2))
D['b'] = D['a']*2 + np.random.randn(N,2)
D['y'] = dm.DataMat(D['a']@np.array((1,1)) + + D['b']@np.array((2,2)) + np.random.randn(N,)/1000,name='y')

# Reverse order of dict
D = dict([D.popitem() for i in range(len(D))])

U,B = fwl_regression(D)
#+end_src

** Linear GMM
#+begin_src python :tangle metrics_miscellany/estimators.py
def linear_gmm(X,y,Z,W=None,return_Omega=False):
    """
    Linear GMM estimator.
    """

    if W is None: # Use 2sls to get initial estimate of W
        b1,Omega1 = tsls(X,y,Z,return_Omega=True)
        W = Omega1.inv
        return linear_gmm(X,y,Z,W=W)
    else:
        n,k = X.shape

        Qxz = X.T@Z/n

        b = lstsq(Qxz@W@Qxz.T,Qxz@W@Z.T@y/n,rcond=None)[0]

        b = pd.Series(b.squeeze(),index=X.columns)

        # Cov matrix
        e = y.squeeze() - X@b

        #Omega = Z.T@(e**2).dg()@Z/n
        # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
        ZTe = Z.T.multiply(e)
        Omega = ZTe@ZTe.T/n

        if return_Omega:
            return b,Omega
        else:
            Vb = (Qxz@Omega.inv@Qxz.T).inv/n
            return b,Vb

#+end_src
*** Linear GMM Tests
#+begin_src python :tangle metrics_miscellany/test/test_linear_gmm.py
import pandas as pd
from metrics_miscellany.estimators import linear_gmm
from datamat import DataMat, DataVec
import numpy as np

def test_linear_gmm(N=500000,tol=1e-2):

    z = DataMat(np.random.standard_normal((N,2)),columns=['z1','z2'])
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.sum(axis=1) + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    b,V = linear_gmm(x,y,z)

    assert np.allclose(b,beta.squeeze(),atol=tol)

    return b,V

if __name__=='__main__':
    b,V = test_linear_gmm()

#+end_src

** Linear GMM with Linear Restrictions
#+begin_src python :tangle metrics_miscellany/estimators.py
def restricted_linear_gmm(X,y,Z,R,r,W=None,return_Omega=False):
    """
    Linear GMM estimator.
    """
    raise NotImplementedError
    if W is None: # Use 2sls to get initial estimate of W
        b1,Omega1 = tsls(X,y,Z,return_Omega=True)
        W = Omega1.inv
        return linear_gmm(X,y,Z,W=W)
    else:
        n,k = X.shape

        Qxz = X.T@Z/n

        b = lstsq(Qxz@W@Qxz.T,Qxz@W@Z.T@y/n,rcond=None)[0]

        b = pd.Series(b.squeeze(),index=X.columns)

        # Cov matrix
        e = y.squeeze() - X@b

        #Omega = Z.T@(e**2).dg()@Z/n
        # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
        ZTe = Z.T.multiply(e)
        Omega = ZTe@ZTe.T/n

        if return_Omega:
            return b,Omega
        else:
            Vb = (Qxz@Omega.inv@Qxz.T).inv/n
            return b,Vb

#+end_src
*** Linear GMM Tests
#+begin_src python :tangle metrics_miscellany/test/test_linear_gmm.py
import pandas as pd
from metrics_miscellany.estimators import linear_gmm
from datamat import DataMat, DataVec
import numpy as np

def test_linear_gmm(N=500000,tol=1e-2):

    z = DataMat(np.random.standard_normal((N,2)),columns=['z1','z2'])
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.sum(axis=1) + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    b,V = linear_gmm(x,y,z)

    assert np.allclose(b,beta.squeeze(),atol=tol)

    return b,V

if __name__=='__main__':
    b,V = test_linear_gmm()

#+end_src

** Factor Regression
We're interested here in multivariate regressions of the form
\[
     Y = XB + F\Lambda + U
\]
where $Y$ and $U$ are $N\times k$, $X$ is $n\times \ell$, $B$ is $\ell\times k$, $F$ is $N\times r$, and \Lambda is $r\times k$.  Only $(Y,X)$ are observed; $F$ is a collection of latent "factors."  The identifying assumptions are that $U$ is orthogonal to $(X,F)$ and $\E F_iF_i^\T = I_r$.  [cite/t:@hansen20:econometrics] describes an iterative approach to estimation, which we implement below.
#+begin_src python :tangle metrics_miscellany/estimators.py
def factor_regression(Y,X,F=None,rank=1,tol=1e-3):

    if rank>1:
        raise NotImplementedError("Factor regression for rank>1 is not reliable.")

    N,k = Y.shape
    def ols(X,Y):
        N,k = Y.shape
        XX = utils.self_inner(X)/N
        XY = utils.matrix_product(X.T,Y)/N
        B = np.linalg.lstsq(XX,XY,rcond=None)[0]
        return pd.DataFrame(B,index=X.columns,columns=Y.columns)

    if F is None:
        B = ols(X,Y)
        F = 0
    else:
        parms = ols(pd.concat([X,F],axis=1),Y)
        L = parms.iloc[-rank:,:]
        B = parms.iloc[:-rank,:]

    lastF = F
    F,s,vt = utils.svd_missing(Y - utils.matrix_product(X,B),max_rank=rank)
    scale = F.std()
    F = F.multiply(1/scale)

    if np.linalg.norm(F-lastF)>tol:
        B,L,F = factor_regression(Y,X,F=F,rank=rank,tol=tol)

    return B,L,F


#+end_src
*** Factor Regression Test
#+begin_src python :tangle metrics_miscellany/test/test_factor_regression.py
import pandas as pd
from scipy import stats
from metrics_miscellany.estimators import factor_regression
from metrics_miscellany import utils
import numpy as np

def generate_multivariate_normal(N,k,V=None,colidx='a'):

    try:
        a = ord(colidx)
        labels = list(map(chr, range(a, a+k)))
    except TypeError:
        labels = range(colidx,colidx+k)

    if V is None:
        D = pd.DataFrame(np.random.randn(k,k),index=labels,columns=labels)
        V = D.T@D
    else:
        V = pd.DataFrame(V,index=labels,columns=labels)

    X = pd.DataFrame(stats.multivariate_normal(cov=V).rvs(N),columns=labels)

    return X

def main(N,k,l,r):

    U = generate_multivariate_normal(N,k,V=np.eye(k),colidx='A')/100

    X = generate_multivariate_normal(N,l)

    a = ord('a')
    A = ord('A')
    rlabels = list(map(chr, range(a, a+l)))
    clabels = list(map(chr, range(A, A+k)))
    B = pd.DataFrame(np.arange(1,l*k+1).reshape(l,k),index=rlabels,columns=clabels)

    F = generate_multivariate_normal(N,r,colidx=0)
    F = F - F.mean()
    scale = F.std()
    F = F.multiply(1/scale)

    L = pd.DataFrame(np.arange(1,k*r+1).reshape(r,k)/10,index=F.columns,columns=U.columns)
    L = L.multiply(scale,axis=0)

    Y = utils.matrix_product(X,B) + utils.matrix_product(F,L) + U

    return Y,X,F,B,L,U

def test_factor_regression(N=1000,k=10,l=2,r=1):
    Y,X,F0,B0,L0,U0 = main(N,k,l,r)
    X['Constant'] = 1

    B,L,F = factor_regression(Y,X,rank=r)

    assert np.linalg.norm(F0-F) < np.linalg.norm(F0)

    assert np.all(Y.var()>(Y-X@B).var())

    assert np.all((Y-X@B).var()>(Y-X@B-F@L).var())

    assert np.linalg.norm((B0-B).dropna())/np.linalg.norm(B0) < 0.01

if __name__ == '__main__':
    test_factor_regression(N=10000,r=1)
 #+end_src

* Order statistics
** Quantiles, with "confidence intervals"
See William Huber's https://stats.stackexchange.com/questions/122001/confidence-intervals-for-median
#+begin_src python :tangle metrics_miscellany/order_statistics.py
from scipy.stats.distributions import binom

def quantile_confidence_intervals(x,q=1/2,minimum_coverage=0.95):
    α = 1-minimum_coverage
    x = x.sort_values()
    n = x.shape[0]
    p = binom(n,q)
    u,cu = [(i,p.cdf(i)) for i in range(1,n) if p.cdf(i)>1-α/2 and p.cdf(i-1)<=1-α/2][0]
    l,cl = [(i-1,p.cdf(i-1)) for i in range(1,n) if p.cdf(i)>α/2 and p.cdf(i-1)<=α/2][0]

    coverage = 1 - cl - (1-cu)
    return (x.iloc[l],x.iloc[u]),coverage
#+end_src
* Kernel Methods
#+begin_src python :tangle metrics_miscellany/kernel_methods.py
import pandas as pd
import datamat as dm
import numpy as np

sqrt3 = np.sqrt(3)  # Avoid repeated evaluation of this for speed...
sqrt2pi = np.sqrt(2*np.pi)

def rectangular(u):
    return (np.abs(u) < sqrt3)/(2*sqrt3)  # Rectangular kernel

def gaussian(u):
    return np.exp(-(u**2)/2)/sqrt2pi # Gaussian kernel

def gram(X,kernel=gaussian,bw=1):
    """
    Construct Gram matrix from vector of data X.
    """
    try:
        idx = X.index
        df = True
        x = X.values
    except AttributeError:
        df = False
        x = X

    assert len(x.shape)==1
    K = kernel((x.reshape((-1,1)) - x.reshape((1,-1)))/bw)

    if df:
        if isinstance(X,(dm.DataVec,dm.DataMat)):
            K = dm.DataMat(K,index=X.index,columns=X.index)
        elif isinstance(X,(pd.Series,pd.DataFrame)):
            K = pd.DataFrame(K,index=X.index,columns=X.index)

    return K



def kernel_regression(X,y,bw,kernel=gaussian):
    """
    Use data (X,y) to estimate E(y|x), using bandwidth bw.
    """
    def mhat(x):
        S = kernel((X-x)/bw) # "Smooths"

        return S.dot(y)/S.sum()

    return mhat

def kernel_regression_variance(X,y,bw,kernel=gaussian):
    """
    Use data (X,y) to estimate E((y-m(x))^2|x), using bandwidth bw.
    """
    # Construct leave-one-out residuals
    K = gram(X,bw=bw,kernel=kernel)
    Km = K - K.dg().dg()

    e2 = (y - (K@y)/K.sum(axis=1))**2
    Ksum = K.sum().sum()

    def sigmahat(x):

        S = kernel((X-x)/bw) # "Smooths"

        return (S**2).dot(e2)/Ksum

    return sigmahat
#+end_src


* GMM
** Procedural interface for GMM estimator.
#+begin_src python :tangle metrics_miscellany/gmm.py
import numpy as np
from . import utils
from . import utils
matrix_product = utils.matrix_product
diag = utils.diag
inv = utils.inv

from scipy.optimize import minimize_scalar, minimize, approx_fprime
from scipy.optimize import minimize as scipy_min
import pandas as pd

from IPython.core.debugger import Pdb

__version__ = "0.3.1"

######################################################
# Beginning of procedural version of gmm routines

def gN(b):
    """Averages of g_j(b).

    This is generic for data, to be passed to gj.
    """
    e = gj(b)

    gN.N,gN.k = e.shape
    gN.N = e.count()  # Allows for possibility of missing data
    # Check to see more obs. than moments.
    assert np.all(gN.N > gN.k), "More moments than observations"

    try:
        return e.mean(axis=0).reshape((-1,1))
    except AttributeError:
        return e.mean(axis=0)

def Omegahat(b):
    e = gj(b)

    # Recenter! We have Eu=0 under null.
    # Important to use this information.
    e = e - e.mean(axis=0)
    sqrtN = np.sqrt(e.count())

    e = e/sqrtN

    ete = matrix_product(e.T,e)

    return ete

def JN(b,W=None):

    if W is None:
        W = utils.inv(Omegahat(b))

    m = gN(b) # Sample moments @ b

    #Pdb().set_trace()

    # Scaling by diag(N) allows us to deal with missing values
    WN = pd.DataFrame(matrix_product(diag(gN.N),W))

    crit = (m.T@WN@m).squeeze()
    assert crit >= 0

    return crit

def minimize(f,b_init=None):
    if b_init is None:
        return minimize_scalar(f).x
    else:
        return scipy_min(f,b_init).x

def one_step_gmm(W=None,b_init=None):

    if b_init is None:
        b_init = 0

    if W is None:
        e = gj(b_init)
        W = pd.DataFrame(np.eye(e.shape[1]),index=e.columns,columns=e.columns)

    assert np.linalg.matrix_rank(W)==W.shape[0]

    b = minimize(lambda b: JN(b,W),b_init=b_init)

    return b, JN(b,W)

def two_step_gmm(b_init=None):

    # First step uses identity weighting matrix
    b1 = one_step_gmm(b_init=b_init)[0]

    # Construct 2nd step weighting matrix using
    # first step estimate of beta
    W2 = utils.inv(Omegahat(b1))

    return one_step_gmm(W=W2,b_init=b1)

def continuously_updated_gmm(b_init=None):

    # First step uses identity weighting matrix
    W = lambda b: utils.inv(Omegahat(b))

    bhat = minimize(lambda b: JN(b,utils.inv(Omegahat(b))),b_init=b_init)

    return bhat, JN(bhat,W(bhat))

def dgN(b):
    """
    Average gradient of gj(b).

    This function provides numerical derivatives.  

    One may wish to override this with a function dgN which returns analytical derivatives.
    """
    gradient = pd.DataFrame(approx_fprime(b,gN),index=gN(b).index)

    return gradient
    

def Vb(b):
    """Covariance of estimator of b.

    Note that one must supply gmm.dgN, the average gradient of gmm.gj at b.
    """
    Q = dgN(b)
    W = pd.DataFrame(matrix_product(diag(gN.N),Omegahat(b)))

    return utils.inv(Q.T@utils.inv(W)@Q)

def print_version():
    print(__version__)

# End of procedural version of gmm routines
######################################################
#+end_src
*** GMM Test
#+begin_src python :tangle metrics_miscellany/test/test_gmm.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import gmm
from numpy.linalg import inv
from scipy.stats import distributions as iid

def dgp(N,beta,gamma,sigma_u,VXZ):
    """Generate a tuple of (y,X,Z).

    Satisfies model:
        y = X@beta + u
        E Z'u = 0
        Var(u) = sigma^2
        Cov(X,u) = gamma*sigma_u^2
        Var([X,Z]|u) = VXZ
        u,X,Z mean zero, Gaussian

    Each element of the tuple is an array of N observations.

    Inputs include
    - beta :: the coefficient of interest
    - gamma :: linear effect of disturbance on X
    - sigma_u :: Variance of disturbance
    - VXZ :: Var([X,Z]|u)
    """

    u = pd.Series(iid.norm.rvs(size=(N,))*sigma_u)

    # "Square root" of VXZ via eigendecomposition
    lbda,v = np.linalg.eig(VXZ)
    SXZ = v@np.diag(np.sqrt(lbda))

    # Generate normal random variates [X*,Z]
    XZ = pd.DataFrame(iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T)

    # But X is endogenous...
    X = XZ.loc[:,0].add(gamma*u,axis=0)
    Z = XZ.loc[:,1:]

    # Calculate y
    y = X*beta + u

    return y,X,Z

def test_gmm(N=10000):

    ## Play with us!
    beta = 1     # "Coefficient of interest"
    gamma = 1    # Governs effect of u on X
    sigma_u = 1  # Note assumption of homoskedasticity
    ## Play with us!

    # Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ

    ell = 4 # Play with me too!

    # Arbitrary (but deterministic) choice for VXZ = [VX Cov(X,Z);
    #                                                 Cov(Z,X) VZ]
    # Pinned down by choice of a matrix A...
    A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1))

    ## Below here we're less playful.

    # Now Var([X,Z]|u) is constructed so guaranteed pos. def.
    VXZ = A.T@A

    Q = -VXZ[1:,[0]]  # -EZX', or generally Edgj/db'

    # Gimme some truth:
    truth = (beta,gamma,sigma_u,VXZ)

    ## But play with Omega if you want to introduce heteroskedascity
    Omega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')

    # Asymptotic variance of optimally weighted GMM estimator:
    AVar_b = inv(Q.T@inv(Omega)@Q)

    data = dgp(N,*truth)

    def gj(b):
        y,X,Z = data
        e = (y.squeeze()-b*X.squeeze())

        Ze = Z.multiply(e,axis=0)

        return Ze

    def dgN(b):
        y,X,Z = data
        return Z.T@X

    gmm.gj = gj
    gmm.dgN = dgN

    b,J = gmm.two_step_gmm()

    assert (b-beta)**2 < 0.01, f"Estimate {b} outside tolerance."

    print(b,J)
    print(gmm.Vb(b))

    return J

if __name__ == '__main__':
    J = []
    for i in range(1000): J.append(test_gmm())

#+end_src
** GMM Class
#+begin_src python :tangle metrics_miscellany/GMM_class.py
from . import gmm
import numpy as np

class GMM(object):

    def __init__(self,gj,data,B,W=None):
        """GMM problem for restrictions E(gj(b0))=0, estimated using data with b0 in R^k.

           - If supplied B is a positive integer k, then
             space taken to be R^k.
           - If supplied B is a k-vector, then
             parameter space taken to be R^k with B a possible
             starting value for optimization.
        """
        self.gj = gj
        gmm.gj = gj  # Overwrite member of gmm module
        self.data = data

        self.W = W

        self.b = None

        try:
            self.k = len(B)
            self.b_init = np.array(B)
        except TypeError:
            self.k = B
            self.b_init = np.zeros(self.k)

        self.ell = gj(self.b_init,self.data).shape[1]

        if type(data) is tuple:
            self.N = data[0].shape[0]
        else:
            self.N = data.shape[0]

        self.minimize = gmm.minimize

    def gN(self,b):
        """Averages of g_j(b).

        This is generic for data, to be passed to gj.
        """
        return gmm.gN(b,self.data)

    def Omegahat(self,b):

        return gmm.Omegahat(b,self.data)

    def JN(self,b,W):

        return gmm.JN(b,W,self.data)

    def one_step_gmm(self,W=None,b_init=None):

        self.b = gmm.one_step_gmm(self.data,W,b_init=self.b_init)[0]

        return self.b

    def two_step_gmm(self):

        self.b = gmm.two_step_gmm(self.data,b_init=self.b_init)[0]
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b

    def continuously_updated_gmm(self):

        est = gmm.continuously_updated_gmm(self.data,b_init=self.b_init)[0]
        self.b = est
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b



if __name__=='__main__':
    #foo = GMM(gmm.gj,
    pass

#+end_src

* Hypothesis Tests
** Chi square tests
#+begin_src python :tangle metrics_miscellany/tests.py
from metrics_miscellany import utils
from scipy import stats
import pandas as pd
import numpy as np

def chi2_test(b,V,var_selection=None,R=None,TEST=False):
    """Construct chi2 test of R'b = 0.

    If R is None then test is b = 0.

    If one wishes to test a hypothesis regarding only a subset of elements of b,
    this subset can be chosen by specifying var_selection as either a query string
    or as a list.
    """

    if var_selection is not None:
        if type(var_selection) is str:
            myb = b.query(var_selection)
        elif type(var_selection) is list:
            myb = b.loc[var_selection]
        else:
            raise(ValueError,"var_selection should be a query string of list of variable names")
    else:
        myb = b


    # Drop parts of matrix not involved in test
    myV = V.reindex(myb.index,axis=0).reindex(myb.index,axis=1)

    myV = utils.cov_nearest(myV,threshold=1e-10)

    if R is not None:
        myV = R.T@myV@R
        myb = R.T@b
        if np.isscalar(myV):
            myV = np.array([[myV]])
            myb = np.array([[myb]])

    if TEST: # Generate values of my that satisfy Var(myb)=Vb and Emyb=0
        myb = myb*0 + stats.multivariate_normal(cov=((1e0)*np.eye(myV.shape[0]) + myV)).rvs().reshape((-1,1))

    # "Invert"...

    L = np.linalg.cholesky(myV)
    y = np.linalg.solve(L.T,myb)

    chi2 = y.T@y

    y = pd.Series(y.squeeze(),index=myb.index)

    return chi2,1-stats.distributions.chi2.cdf(chi2,df=len(myb))


#+end_src

*** Test of chi2_test
#+begin_src python :tangle metrics_miscellany/test/test_chi2_test.py
import pandas as pd
from scipy import stats
from metrics_miscellany import tests
import numpy as np

def main():

    labels = ['a','b']
    D = pd.DataFrame([[2,1],[2,2]],index=labels,columns=labels)
    D.index.name = 'Variable'
    D.columns.name = 'Variable'

    V = D.T@D

    b = pd.DataFrame(stats.multivariate_normal(cov=V).rvs(),index=labels)
    b.index.name = 'Variable'

    return tests.chi2_test(b,V,"Variable in ['a']")

def test_chi2():
    p = []
    m = 1000
    for i in range(m):
        p.append(main()[1])

    p = pd.Series([x[0][0] for x in p]).squeeze()

    X = np.linspace(.05,.95,10)
    assert np.linalg.norm(p.quantile(X) - X)/len(X) < 1e-1

if __name__ == '__main__':
    test_chi2()
 #+end_src
** Skillings-Mack Test (Generalization of Friedman Test)
This implements a version of the test proposed in [cite/t:@skillings-mack81], which generalizes the Friedman rank test to the case in which data is incomplete.  Because the Friedman test is a special case, we also create a =friedman= test.
#+begin_src python :tangle metrics_miscellany/tests.py
def skillings_mack(df,bootstrap=False):
    """
    Non-parametric test of correlation across columns of df.

    Algorithm from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2761045/
    """
    def construct_statistic(R,kay):
        """
        Once we have ranks, construct SM statistic
        """
        # Fill missing ranks with (k_i+1)/2
        R = R.where(~np.isnan(R),(kay+1)/2,axis=1)

        # Construct adjusted observation matrix
        A = R.subtract((kay.values+1)/2,axis=1)@np.sqrt(12/(kay.values+1))

        # Count of observations in both columns k and l
        O = ~np.isnan(X)+0.

        Sigma = np.eye(O.shape[0]) - O@O.T

        # Delete diagonal
        Sigma = Sigma - np.diag(np.diag(Sigma))

        # Add minus column sums to diagonal
        Sigma = Sigma - np.diag(Sigma.sum())

        return A.T@np.linalg.pinv(Sigma)@A

    # Drop any rows with only one column
    X = df.loc[df.count(axis=1)>0]

    n,k = X.shape

    # Counts of obs per row ("treatments")
    kay = X.count(axis=0)

    # Counts of obs per column ("blocks")
    en = X.count(axis=1)

    R = X.rank(axis=0)

    SM = construct_statistic(R,kay)

    if not bootstrap:
        p = 1-stats.distributions.chi2.cdf(SM,df=n-1)
    else:
        if bootstrap == True:
            tol = 1e-03
        else:
            tol = bootstrap

        SE = 0
        lastSE = np.inf
        its = 0
        sms = []
        while (its < 30) or (np.abs(SE-lastSE) > tol):
            lastSE = SE
            scrambled = pd.DataFrame(np.apply_along_axis(np.random.permutation,axis=0,arr=R.values),
                                     index=R.index,columns=R.columns)
           
            sms.append(construct_statistic(scrambled,kay))
            SE = np.std(sms)
            its += 1
        p = np.mean(sms>SM)

    return SM,p

friedman = skillings_mack
#+end_src
*** Test of Skillings Mack
#+begin_src python :tangle metrics_miscellany/test/test_skillings_mack.py
import pandas as pd
import numpy as np
from metrics_miscellany import tests

def test_sm_against_R():
    """This is an example given in https://cran.r-project.org/web/packages/Skillings.Mack/Skillings.Mack.pdf
    """
    X = pd.DataFrame([[3.2, 3.1, 4.3, 3.5, 3.6, 4.5, np.nan, 4.3, 3.5],
                      [4.1, 3.9, 3.5, 3.6, 4.2, 4.7, 4.2, 4.6, np.nan],
                      [3.8, 3.4, 4.6, 3.9, 3.7, 3.7, 3.4, 4.4, 3.7],
                      [4.2, 4.,  4.8, 4., 3.9, np.nan, np.nan, 4.9, 3.9]])

    # This value of SM statistic matches that from R Skill.Mack routine
    assert np.allclose(tests.skillings_mack(X)[0],15.493)

def test_sm_type1():
    # Now try a random matrix
    ps = pd.Series([tests.skillings_mack(pd.DataFrame(np.random.rand(100,10)),bootstrap=0.02)[1] for i in range(100)])

    # p values should be distributed uniformly, with mean of 1/2
    tstat = (ps.mean()-1/2)/ps.std()

    assert np.abs(tstat)<2

if __name__=='__main__':
    test_sm_against_R()
    test_sm_type1()
#+end_src
** Randomization Inference
   Suppose we want to estimate a linear regression
   \[
       y = \alpha + X\beta + W\gamma + u.
   \]

   We obtain estimates $(b,V_b)$ of the coefficients $\beta$ and
   corresponding covariance matrix.  We want to be able to conduct a
   test of the hypothesis $R'\beta=0$.

   The idea here is to use resampling of just the variables $X$
   without replacement as a way of drawing inferences regarding
   \beta.  In particular, we randomly permute the rows of $X$,
   creating a new variable $P$, and estimate
   \[
       y = \alpha + P\delta + W\gamma + u,
   \]
   yielding estimates $(d,V_d)$ for the coefficients $\delta$ and the
   covariance matrix of these estimates.

   Note that $R'\E d = 0$ by construction, for any set of linear
   restructions $R$.  The linear restrictions themselves suggest a
   $\chi^2$ test; denote this statistic by $T(R,d,V)$.  We repeat the
   permute-estimate-test cycle many times.  Then the proportion of
   times that the test statistic associated with the test of
   $$R'(\beta-\hat\delta)>0$ gives us a \(p\)-value associated with a
   test of the null hypothesis that $\beta>c$.  A two-sided test can
   be constructed from the absolute difference in absolute values;
   i.e., $|\beta - \delta|>c$.

#+begin_src python :tangle metrics_miscellany/tests.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import ols

def randomization_inference(vars,X,y,permute_levels=None,R=None,tol=1e-3,VERBOSE=False,return_draws=False):
    """
    Return p-values associated with hypothesis that coefficients
    associated with vars are jointly equal to zero.

    Ethan Ligon                                       June 2021
    """

    assert np.all([v in X.columns for v in vars]), "vars must correspond to columns of X."

    b,V = ols(X,y)

    beta = b.squeeze()[vars]
    chi2 = chi2_test(beta,V,R=R)[0]

    last = np.inf
    p = 0
    i = 0
    Chi2 = []
    while (np.linalg.norm(p-last)>tol) or (i < 30):
        last = p
        if permute_levels is None:
            P= pd.DataFrame(np.random.permutation(X.loc[:,vars]),index=X.index,columns=vars)
        else:
            levels = X.index.names
            fixed = X.index.names.difference(permute_levels)
            P = pd.DataFrame(X.loc[:,vars].unstack(fixed).sample(frac=1).stack(fixed).values,index=X.index,columns=vars)

        myX = pd.concat([X.loc[:,X.columns.difference(vars)],P],axis=1)
        b,V = ols(myX,y)
        Chi2.append(chi2_test(b.squeeze()[vars],V,R=R)[0])
        p = (chi2<Chi2[-1])/(i+1) + last*i/(i+1)
        i += 1
        if VERBOSE: print("Latest chi2 (randomized,actual,p): (%6.2f,%6.2f,%6.4f)" % (Chi2[-1],chi2,p))

    if return_draws:
        return p,pd.Series(Chi2)
    else:
        return p

#+end_src

*** Test of randomization inference
#+begin_src ipython :tangle metrics_miscellany/test/test_randomization_inference.py
import pandas as pd
import scipy.stats.distributions as dists
from metrics_miscellany import estimators, tests
import matplotlib.pyplot as plt

n=1000
p = 0.5
# Generate contextual variables; probability of being female is p
C = pd.DataFrame({'Female':dists.binom(1,p).rvs(size=n)})
C['Male'] = 1-C

delta = pd.Series({"Female":1.,"Male":0.5})

T1 = pd.Series(dists.norm.rvs(size=n),name='Treatment1')
T2 = pd.Series(dists.norm.rvs(size=n),name='Treatment2')

# Interactions:
TC = C.multiply(T1,axis=0)
TC.columns = ['TxFemale','TxMale']

# Construct RHS matrix
X = pd.concat([T1,T2,C,TC],axis=1).iloc[:,:-1]
dC = C@delta

# Generate outcome y with *no* treatment effect, to look for Type I errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = dC + epsilon
Y.name = 'outcome'

p_i = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Generate outcome y with uniform treatment effect, to look for Type II errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = T1 + T2 + dC + epsilon
Y.name = 'outcome'

p_ii = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Test Treatment1 == Treatment2
R = pd.DataFrame({'Coefficients':[1,-1]},index=['Treatment1','Treatment2'])

p_iii = tests.randomization_inference(['Treatment1','Treatment2'],X.drop('TxFemale',axis=1),Y,R=R,VERBOSE=True)

#+end_src

** Maunchy test of sphericity
This test asks whether, given a sample covariance matrix $S$, one can
reject the hypothesis that the population covariance matrix
$\Sigma=\sigma I$; i.e., whether the random vector with variance
matrix $\Sigma$ has a spherical distribution or not (note that the
test is obtained under the assumption that the random vectors are
normally distributed), and is due to Maunchy (1940)[fn:: See
cite:muirhead82 p. 334.].
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def maunchy(C,N):
    """Given a sample covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix is proportional to the identity matrix.
    """

    raise NotImplementedError

    m = C.shape[0]

    V = np.linalg.det(C)/((np.trace(C)/m)**m)

    rho = 1 - (2*m**2 + m + 2)/(6*m*(N-1))

    w2 = (m-1)*(m-2)*(m+2)*(2*m**3 + 6*m**2 + 3*m + 2)/(288*(m**2) * ((N-1)**2) * rho**2)

    gamma = (((N-1)*rho)**2)*w2

    x2 = -2*(N-1)*rho*np.log(V)  # Chi-squared statistic

    df = (m+2)*(m-1)/2

    px2 = chi2.cdf(x2,df)

    p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** COMMENT Test of Maunchy
#+begin_src python :tangle metrics_miscellany/test/test_maunchy.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
import matplotlib.pyplot as plt

N = 300
k = 10

Chi2 = []
P = []
for m in range(1000):
    X = iid.norm.rvs(size=(N,k))

    C = np.cov(X,rowvar=False)
    x,p = tests.maunchy(C,N)
    Chi2.append(x)
    P.append(p)

df = (k+2)*(k-1)/2


range = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.plot(range,[iid.chi2.pdf(x,df) for x in range])[0]
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
#print(
#+end_src
** Inference on eigenvalues
Suppose we wish to test whether a covariance matrix has a structure $\Sigma =
\Lambda\Lambda^\T + \lambda I$, where $\Lambda$ is rank $r$.  This
structure is often assumed in exact factor models, for example.
cite:srivastava-khastri79 (\S 9.5) suggest a simple likelihood ratio test,
implemented here.
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def kr79(C,q,N):
    """Given a sample mxm covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix has last q eigenvalues equal or not, where q+k=m.
    """

    m = C.shape[0]

    l = np.linalg.eigvalsh(C)  # eigenvalues in *ascending* order

    Q = (np.prod(l[:q])/(np.mean(l[:q])**q))**(N/2) # LR test statistic

    x2 = -2*np.log(Q)  # Chi-squared statistic

    df = (q-1)*(q+2)/2

    px2 = chi2.cdf(x2,df)

    #p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** Test of kr79
#+begin_src python :tangle metrics_miscellany/test/test_kr79.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
from scipy import stats
import matplotlib.pyplot as plt

N = 10000
m = 10
r = 3
q = m - r

# Build covariance matrix for "systematic" variation
Sigma = iid.norm.rvs(size=(m,r))
Sigma = Sigma@Sigma.T   # Positive definite

l,v = np.linalg.eigh(Sigma)
l = np.maximum(l,0)

Ssqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Sigma,Ssqrt@Ssqrt.T)

# Build covariance matrix for errors.
## Use (something proportional to) identity for tests of size, otherwise tests of power.
#Psi = 10.0*np.eye(m)  + np.diag(range(1,m+1))/(m**3)
Psi = np.eye(m)
Psi = Psi@Psi.T   # Positive definite

l,v = np.linalg.eigh(Psi)
l = np.maximum(l,0) 

Psisqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Psi,Psisqrt@Psisqrt.T)


Chi2 = []
P = []
for s in range(1000):
    X = iid.norm.rvs(size=(N,m))@Ssqrt.T
    e = iid.norm.rvs(size=(N,m))@Psisqrt.T

    C = np.cov(X + e,rowvar=False)
    x,p = tests.kr79(C,q,N)
    Chi2.append(x)
    P.append(p)

df = (q+2)*(q-1)/2

xrange = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
ax.plot(xrange,[iid.chi2.pdf(x,df) for x in xrange])

assert stats.kstest(P,stats.distributions.uniform.cdf).pvalue>0.01
#+end_src
** Cragg-Donald Reduced Rank Test
#+begin_src python :tangle metrics_miscellany/tests.py
def cragg_donald(X,Q):
    """
    Cragg-Donald (1997) test for weak instruments.

    In multivariate regression

    X = Q\Pi + v

    We test null hypothesis that the rank of \Pi is less than the rank of X.

    Returns a statistic distributed chi2, along with relevant p-value
    """
    teststat = (X.T@X.resid(Q)).inv@(X.T@X.proj(Q))

    teststat = teststat.eig()[0].min()

    n,k = Q.shape

    teststat = (n-k)*teststat

    pvalue = 1 - chi2(n-X.shape[1]+1).cdf(teststat)

    return teststat,pvalue

#+end_src
*** Test of Cragg-Donald
#+begin_src python :tangle metrics_miscellany/test/test_cragg_donald.py
"""Check size of cragg_donald reduced rank test.
"""
import datamat as dm
from metrics_miscellany.tests import cragg_donald
from scipy.stats.distributions import norm, chi2
import numpy as np

def test_size(n=1000,m=8,l=10,tol=1e-4):
    Pi = norm.rvs(size=(l,m))
    Z = dm.DataMat(norm.rvs(size=(n,l)))

    its = 0
    laststat = -1
    stat = 0
    Stats = []
    Ps = []
    while its<=30 or np.abs(stat-laststat)>tol:
        its += 1
        laststat = stat
        U = dm.DataMat(norm.rvs(size=(n,m)))
        X = Z@Pi + U
        s,p = cragg_donald(X,Z)
        Stats.append(s)
        Ps.append(p)
        stat = p/its + stat*(its-1)/its
        if its % 12 == 1:
            print(f"Iteration {its}, alpha={stat:4.4f}")



#+end_src


* Utils
** Some QR Tricks
*** QR Decomposition
Begin with wrapping the numpy qr decomposition to return dataframes.
#+begin_src python  :tangle metrics_miscellany/utils.py
import numpy as np
import pandas as pd

def qr(X):
    """
    Pandas-friendly QR decomposition.
    """
    assert X.shape[0]>=X.shape[1]

    Q,R = np.linalg.qr(X)
    Q = pd.DataFrame(Q,index=X.index, columns=X.columns)
    R = pd.DataFrame(R,index=X.columns, columns=X.columns)

    return Q,R
#+end_src

*** Leverage
Now, use the fact that the leverage of different observations in $X$
are the sums of squares of rows of $Q$ in the $QR$ decomposition
#+begin_src python :tangle metrics_miscellany/utils.py

def leverage(X):
    """
    Return leverage of observations in X (the diagonals of the hat matrix).
    """

    Q = qr(X)[0]

    return (Q**2).sum(axis=1)
#+end_src

*** Hat factory

Now construct a factory that returns a function to put the "hat"
on y.  Though mathematically this looks like $X(X'X)^{-1}X'=QQ'$ in
practice we don't want to construct an $N\times N$ matrix like this,
as it's often too expensive.

#+begin_src python :tangle metrics_miscellany/utils.py
def hat_factory(X):
    """
    Return a function hat(y) that returns X(X'X)^{-1}X'y.

    This is the least squares prediction of y given X.

    We use the fact that  the hat matrix is equal to QQ',
    where Q comes from the QR decomposition of X.
    """
    Q = qr(X)[0]

    def hat(y):
        return Q@(Q.T@y)

    return hat
#+end_src


**  "Fixing" matrices that aren't quite positive definite
#+begin_src python :tangle metrics_miscellany/utils.py
from statsmodels.stats.correlation_tools import cov_nearest as _cov_nearest
import pandas as pd

def cov_nearest(V,threshold=1e-12):
    """
    Return a positive definite matrix which is "nearest" to the symmetric matrix V,
    with the smallest eigenvalue not less than threshold.
    """
    s,U = np.linalg.eigh((V+V.T)/2) # Eigenvalue decomposition of symmetric matrix

    s = np.maximum(s,threshold)

    return V*0 + U@np.diag(s)@U.T  # Trick preserves attributes of dataframe V
#+end_src
** Trimming
#+begin_src python :tangle metrics_miscellany/utils.py
import pandas as pd
import numpy as np

def trim(df,alpha):
    """Trim values below alpha quantile and above (1-alpha) quantile.

    This maps individual extreme elements of df to NaN.
    """
    xmin = df.quantile(alpha)
    xmax = df.quantile(1-alpha)
    return df.where((df>=xmin)*(df<=xmax),np.nan)
#+end_src

* Random
** Random permutations of dataframes
#+begin_src python :tangle metrics_miscellany/random.py
import numpy as np
import pandas as pd

def permutation(df,columns=None,permute_levels=None):
    """Randomly permute rows of df[columns] at permute_levels.
    """

    df = pd.DataFrame(df) # Make sure we have a DataFrame.

    if columns is None: columns = df.columns

    if permute_levels is None:
        P = pd.DataFrame(np.random.permutation(df.loc[:,columns]),index=df.index,columns=columns)
    else:
       fixed = df.index.names.difference(permute_levels)
       P = pd.DataFrame(df.loc[:,columns].unstack(fixed).sample(frac=1).stack(fixed).values,index=df.index,columns=columns)

    return P

#+end_src
*** Test permutation
#+begin_src python :tangle metrics_miscellany/test/test_permutation.py
import numpy as np
import pandas as pd
from metrics_miscellany import random

T = pd.Series(np.random.rand(10)>.5)

df = pd.DataFrame({'a':T,'b':T}).stack()
df = df + 0
df.index.names = ['i','t']

p = random.permutation(df,permute_levels=['i'])

assert np.all(p.unstack('t').std(axis=1)==0)

#+end_src
