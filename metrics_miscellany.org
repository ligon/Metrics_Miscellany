* DataFrame Utils
Here we provide various utils meant to operate on pandas DataFrames and Series.  Note that because datamat classes inherit from pandas these functions can be used with the datamat classes as well.
** Unary Matrix Operators
#+begin_src python :tangle metrics_miscellany/utils.py
import numpy as np
from scipy import sparse as scipy_sparse
import pandas as pd

def inv(A):
    """Inverse of square pandas DataFrame."""
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.inv(A)
    return pd.DataFrame(B,columns=A.columns,index=A.index)

def pinv(A):
    """Moore-Penrose pseudo-inverse of A.

    >>> A = pd.DataFrame([[1,2,3],[4,5,6]])
    >>> np.allclose(A@pinv(A),np.eye(2))
    True
    """
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.pinv(A)
    return pd.DataFrame(B,columns=A.index,index=A.columns)

def svd(A,hermitian=False):
    """Singular value composition into U@S.dg@V.T."""
    idx = A.index
    cols = A.columns
    u,s,vt = np.linalg.svd(A,compute_uv=True,full_matrices=False,hermitian=hermitian)
    u = pd.DataFrame(u,index=idx)
    vt = pd.DataFrame(vt,columns=cols)
    s = pd.Series(s)

    return u,s,vt

def eig(A,hermitian=False):
    """Singular value composition into U@S.dg@V.T."""
    idx = A.index
    cols = A.columns
    if hermitian:
        s2,u = np.linalg.eigh(A)
    else:
        s2,u = np.linalg.eig(A)

    s2 = np.flip(s2)
    u = np.fliplr(u)

    u = pd.DataFrame(u,index=idx,columns=cols)
    s2 = pd.Series(s2.squeeze())

    return s2,u

def diag(X,sparse=True):

    try:
        assert X.shape[0] == X.shape[1]
        d = pd.Series(np.diag(X),index=X.index)
    except IndexError: # X is a series?
        if sparse:
            # We can wind up blowing ram if not careful...
            d = scipy_sparse.diags(X.values)
            d = pd.DataFrame.sparse.from_spmatrix(d,index=X.index,columns=X.index)
        else:
            raise NotImplementedError
    except AttributeError: # Not a pandas object?
        d = np.diag(X)

    return d



#+end_src

** Binary Matrix Products
#+begin_src python :tangle metrics_miscellany/utils.py
def outer(S,T):
    """Outer product of two series (vectors) S & T.
    """
    return pd.DataFrame(np.outer(S,T),index=S.index,columns=T.index)

def matrix_product(X,Y,strict=False,fillmiss=True):
    """Compute matrix product X@Y, allowing for possibility of missing data.

    The "strict" flag if set requires that the names of levels of indices that vary for columns of X be in the intersection of names of levels of indices that vary for rows of Y.
    """

    if strict and not all(X.columns==Y.index):  # Columns and Indices don't match.
        X.columns = drop_vestigial_levels(X.columns)
        Y.index = drop_vestigial_levels(Y.index)

    if fillmiss:
        X = X.fillna(0)
        Y = Y.fillna(0)

    prod = np.dot(X,Y) #.squeeze()

    if len(prod.shape)==1 or prod.shape[1]==1:
        out = pd.Series(prod.squeeze(),index=X.index)
    else:
        try:
            cols = Y.columns
        except AttributeError:
            cols = None
        out = pd.DataFrame(prod,index=X.index,columns=cols)

    return out

def self_inner(X,min_obs=None):
    """Compute inner product X.T@X, allowing for possibility of missing data."""
    n,m=X.shape

    if n<m:
        axis=1
        N=m
    else:
        axis=0
        N=n

    xbar=X.mean(axis=axis)

    if axis:
        C=(N-1)*X.T.cov(min_periods=min_obs)
    else:
        C=(N-1)*X.cov(min_periods=min_obs)

    return C + N*np.outer(xbar,xbar)

def kron(A,B,sparse=False):
    if sparse:
        from scipy.sparse import kron

    if isinstance(A,pd.DataFrame):
        a = A.values
        if isinstance(B,pd.DataFrame):
            columns = pd.MultiIndex.from_tuples([(*i,*j) for i in A.columns for j in B.columns])
            b = B.values
        else:
            columns = A.columns.remove_unused_levels()
            b = B.values.reshape((-1,1))
    elif isinstance(B,pd.DataFrame):
        columns = B.columns.remove_unused_levels()
        a = A.values.reshape((-1,1))
        b = B.values

    index = pd.MultiIndex.from_tuples([(*i,*j) for i in A.index for j in B.index],
                                      names=A.index.names+B.index.names)

    if sparse:
        a = kron(a,b)
        return pd.DataFrame.sparse.from_spmatrix(a,columns=columns,index=index)
    else:
        a = np.kron(a,b)
        return pd.DataFrame(a,columns=columns,index=index)

#+end_src
*** Binary Operation Tests
#+begin_src python :tangle metrics_miscellany/test/test_index_multiplication.py
import metrics_miscellany.datamat as dm
import pandas as pd
import numpy as np

idx = pd.MultiIndex.from_tuples([(0,0,0),(0,0,1),(1,0,0),(1,0,1)],names=['i','j','k'])
X = dm.DataMat([[1,2,3,4]],columns=idx,idxnames=['l'])
Y = dm.DataMat([[1,2,3,0]],columns=idx.droplevel('j'),idxnames='m').T

assert (X@Y).index.names == ['l']

(X.matmul(Y,strict=True))
#+end_src
#+begin_src python :tangle metrics_miscellany/test/test_binary_ops.py
import metrics_miscellany.datamat as dm
import pandas as pd
import numpy as np

def test_matmul(A,B):
    C = A@B
    assert isinstance(C,type(A))

    return C

def test_matmul_matvec(A,b):
    C = A@b
    assert isinstance(C,type(b))

    return C

if __name__=='__main__':
    A = np.array([[1,2],[3,4]])
    B = np.array([[1,1]]).T
    Cnp = test_matmul(A,B)

    A = pd.DataFrame([[1,2],[3,4]])
    B = pd.DataFrame([[1,1]]).T
    Cpd = test_matmul(A,B)

    A = dm.DataMat([[1,2],[3,4]])
    B = dm.DataMat([[1,1]]).T
    Cdm = test_matmul(A,B)

    b = dm.DataVec([1,1])
    c = test_matmul_matvec(A,b)

    b = pd.Series([1,1])
    c = test_matmul_matvec(A,b)
#+end_src
** Matrix Decompositions
#+begin_src python :tangle metrics_miscellany/utils.py
def heteropca(C,r=1,max_its=50,tol=1e-3,verbose=False):
    """Estimate r factors and factor weights of covariance matrix C."""
    from scipy.spatial import procrustes

    N = C - np.diag(np.diag(C))

    ulast = np.zeros((N.shape[1],r))
    u = np.zeros((N.shape[1],r))
    u[0,0] = 1
    ulast[-1,0] = 1

    t = 0

    while procrustes(u,ulast)[-1] >tol and t<max_its:
        ulast = u

        u,s,vt = np.linalg.svd(N,full_matrices=False,hermitian=True)

        s = s[:r]
        u = u[:,:r]

        Ntilde = u[:,:r]@np.diag(s[:r])@vt[:r,:]

        N = N - np.diag(np.diag(N)) + np.diag(np.diag(Ntilde))

        t += 1

        if t==max_its:
            warnings.warn("Exceeded maximum iterations (%d)" % max_its)
        if verbose: print(f"Iteration {t}, u[0,:r]={u[0,:r]}.")

    return u,s

def svd_missing(A,max_rank=None,min_obs=None,heteroskedastic=False,verbose=False):
    """Singular Value Decomposition with missing values

    Returns matrices U,S,V.T, where A~=U*S*V.T.

    Inputs:
        - A :: matrix or pd.DataFrame, with NaNs for missing data.

        - max_rank :: Truncates the rank of the representation.  Note
                      that this impacts which rows of V will be
                      computed; each row must have at least max_rank
                      non-missing values.  If not supplied rank may be
                      truncated using the Kaiser criterion.

        - min_obs :: Smallest number of non-missing observations for a
                     row of U to be computed.

        - heteroskedastic :: If true, use the "heteroPCA" algorithm
                       developed by Zhang-Cai-Wu (2018) which offers a
                       correction to the svd in the case of
                       heteroskedastic errors.  If supplied as a pair,
                       heteroskedastic[0] gives a maximum number of
                       iterations, while heteroskedastic[1] gives a
                       tolerance for convergence of the algorithm.

    Ethan Ligon                                        September 2021

    """
    # Defaults; modify by passing a tuple to heteroskedastic argument.
    max_its=50
    tol = 1e-3

    P = self_inner(A,min_obs=min_obs) # P = A.T@A

    sigmas,v=np.linalg.eigh(P)

    order=np.argsort(-sigmas)
    sigmas=sigmas[order]

    # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
    v=v[:,order]
    v=v[:,sigmas>0]
    s=np.sqrt(sigmas[sigmas>0])

    if max_rank is not None and len(s) > max_rank:
        v=v[:,:max_rank]
        s=s[:max_rank]

    r=len(s)

    if heteroskedastic: # Interpret tuple
        try:
            max_its,tol = heteroskedastic
        except TypeError:
            pass
        Pbar = P.mean()
        v,s = heteropca(P-Pbar,r=r,max_its=max_its,tol=tol,verbose=verbose)

    if A.shape[0]==A.shape[1]: # Symmetric; v=u
        return v,s,v.T
    else:
        vs=v@np.diag(s)

        u=np.zeros((A.shape[0],len(s)))
        for j in range(A.shape[0]):
            a=A.iloc[j,:].values.reshape((-1,1))
            x=np.nonzero(~np.isnan(a))[0] # non-missing elements of vector a
            if len(x)>=r:
                u[j,:]=(np.linalg.pinv(vs[x,:])@a[x]).reshape(-1)
            else:
                u[j,:]=np.nan

    s = pd.Series(s)
    u = pd.DataFrame(u,index=A.index)
    v = pd.DataFrame(v,index=A.columns)

    return u,s,v

def sqrtm(A,hermitian=False):
    """
    Return a positive semi-definite square root for the matrix A.

    NB: A must itself be positive semi-definite.
    """
    u,s,vt = svd(A,hermitian=hermitian)

    if np.any(s<0):
        raise ValueError("Matrix must be positive semi-definite.")

    return u@np.diag(np.sqrt(s))@vt

def cholesky(A):
    """
    Cholesky decomposition A = L@L.T; return lower-triangular L.
    """
    L = np.linalg.cholesky(A)
    return pd.DataFrame(L,index=A.index,columns=A.columns)
#+end_src
** DataFrame/Mat Manipulations
#+begin_src python :tangle metrics_miscellany/utils.py
from cfe.df_utils import use_indices
from pandas import concat, get_dummies, MultiIndex

def dummies(df,cols,suffix=False):
    """From a dataframe df, construct an array of indicator (dummy) variables,
    with a column for every unique row df[cols]. Note that the list cols can
    include names of levels of multiindices.

    The optional argument =suffix=, if provided as a string, will append suffix
    to column names of dummy variables. If suffix=True, then the string '_d'
    will be appended.
    """
    idxcols = list(set(df.index.names).intersection(cols))
    colcols = list(set(cols).difference(idxcols))

    v = concat([use_indices(df,idxcols),df[colcols]],axis=1)

    usecols = []
    for s in idxcols+colcols:
        usecols.append(v[s].squeeze())

    tuples = pd.Series(list(zip(*usecols)),index=v.index)

    v = get_dummies(tuples).astype(int)

    if suffix==True:
        suffix = '_d'

    if suffix!=False and len(suffix)>0:
        columns = [tuple([str(c)+suffix for c in t]) for t in v.columns]
    else:
        columns = v.columns
        
    v.columns = MultiIndex.from_tuples(columns,names=idxcols+colcols)

    return v
#+end_src
** Index utilities
#+begin_src python :tangle metrics_miscellany/utils.py
import pandas as pd
def drop_vestigial_levels(idx,axis=0,both=False):
    """
    Drop levels that don't vary across the index.

    >>> idx = pd.MultiIndex.from_tuples([(1,1),(1,2)],names=['i','j'])
    >>> drop_vestigial_levels(idx)
    Index([1, 2], dtype='int64', name='j')
    """
    if both:
        return drop_vestigial_levels(drop_vestigial_levels(idx,axis=1))

    if axis==1:
        idx = idx.T

    if isinstance(idx,(pd.DataFrame,pd.Series)):
        df = idx
        idx = df.index
        HumptyDumpty = True
    else:
        HumptyDumpty = False

    try:
        l = 0
        L = len(idx.levels)
        while l < L:
            if len(set(idx.codes[l]))<=1:
                idx = idx.droplevel(l)
                L -= 1
            else:
                l += 1
                if l>=L: break
    except AttributeError:
        pass

    if HumptyDumpty:
        df.index = idx
        idx = df
        if axis==1:
            idx = idx.T

    return idx

#+end_src

* datamat
** DataVec class
#+begin_src python :tangle metrics_miscellany/datamat.py
import pandas as pd
import numpy as np
from metrics_miscellany.utils import matrix_product, diag
from metrics_miscellany.utils import inv as matrix_inv
from metrics_miscellany.utils import pinv as matrix_pinv
import metrics_miscellany.utils as utils
from functools import cached_property
from scipy import sparse as scipy_sparse

class DataVec(pd.Series):
    __pandas_priority__ = 5000

    def __init__(self, data=None,**kwargs):
        """Create a DataVec.

        Inherit from :meth: `pd.Series.__init__`.

        Additional Parameters
        ---------------------
        idxnames
                (List of) name(s) for levels of index.
        """
        if 'idxnames' in kwargs.keys():
            idxnames = kwargs.pop('idxnames')
        else:
            idxnames = None

        if data is not None:
            try:
                if len(data.shape)==2 and 1 in data.shape:
                    data = data.squeeze()
            except (AttributeError,):
                pass

        super(DataVec, self).__init__(data=data,**kwargs)

        # Always work with multiindex
        try:
            self.index.levels
        except AttributeError:
            self.index = pd.MultiIndex([self.index],[range(len(self.index))],names=self.index.names)

        if idxnames is None:
            idxnames = list(self.index.names)
            it = 0
            for i,name in enumerate(idxnames):
                if name is None:
                    idxnames[i] = f"_{it:d}"
                    it += 1
        elif isinstance(idxnames,str):
            idxnames = [idxnames]

        self.index.names = idxnames

    def __getitem__(self,key):
        """v.__getitem__(k) == v[k]

        >>> v = DataVec({'a':1,'b':2})
        >>> v['a']
        1
        """
        try:
            return super().__getitem__(key)
        except KeyError: # Perhaps key was for an index?
            return __getitem__(self,(key,))

    @property
    def _constructor(self):
        return DataVec

    @property
    def _constructor_expanddim(self):
        return DataMat

    # Unary operations
    def dg(self,sparse=True):
        """Return"""
        if sparse:
            # We can wind up blowing ram if not careful...
            d = scipy_sparse.diags(self.values)
            return DataMat(pd.DataFrame.sparse.from_spmatrix(d,index=self.index,columns=self.index))
        else:
            return DataMat(np.diag(self.values),index=self.index,columns=self.index)

    def norm(self,ord=None,**kwargs):
        return np.linalg.norm(self,ord,**kwargs)

    # Binary operations
    def outer(self,other):
        """Outer product of two series (vectors).
        """
        return DataMat(np.outer(self,other),index=self.index,columns=other.index)

    def proj(self,other):
        """Projection of self on other."""
        b = other.lstsq(self)
        return other@b


    # Other manipulations
    def concat(self,other,axis=0,levelnames=False,toplevelname='v',suffixer='_',
               drop_vestigial_levels=False,**kwargs):
        p = DataMat(self)
        out = p.concat(other,axis=axis,
                       levelnames=levelnames,
                       toplevelname=toplevelname,
                       suffixer=suffixer,
                       drop_vestigial_levels=drop_vestigial_levels,
                       ,**kwargs)

        if axis==0: out = out.squeeze()

        return out


#+end_src
** DataMat class
#+begin_src python :tangle metrics_miscellany/datamat.py

class DataMat(pd.DataFrame):
    __pandas_priority__ = 6000

    def __init__(self, *args, **kwargs):
        """Create a DataMat.

        Inherit from :meth: `pd.DataFrame.__init__`.

        Additional Parameters
        ---------------------
        idxnames
                (List of) name(s) for levels of index.
        colnames
                (List of) name(s) for levels of columns.
        name
                String naming DataMat object.
        """
        if 'idxnames' in kwargs.keys():
            idxnames = kwargs.pop('idxnames')
        else:
            idxnames = None

        if 'colnames' in kwargs.keys():
            colnames = kwargs.pop('colnames')
        else:
            colnames = None

        if 'name' in kwargs.keys():
            name = kwargs.pop('name')
        else:
            name = None

        super(DataMat, self).__init__(*args,**kwargs)

        self.name = name

        # Always work with multiindex
        try:
            self.index.levels
        except AttributeError:
            self.index = pd.MultiIndex([self.index],[range(len(self.index))],names=self.index.names)

        try:
            self.columns.levels
        except AttributeError:
            self.columns = pd.MultiIndex([self.columns],[range(len(self.columns))],names=self.columns.names)


        if idxnames is None:
            idxnames = list(self.index.names)
            it = 0
            for i,name in enumerate(idxnames):
                if name is None:
                    idxnames[i] = f"_{it:d}"
                    it += 1
        elif isinstance(idxnames,str):
            idxnames = [idxnames]

        self.index.names = idxnames

        if colnames is None:
            colnames = list(self.columns.names)
            it = 0
            for i,name in enumerate(colnames):
                if name is None:
                    colnames[i] = f"_{it:d}"
                    it += 1
        elif isinstance(colnames,str):
            colnames = [colnames]

        self.columns.names = colnames

    def __getitem__(self,key):
        """X.__getitem__(k) == X[k]

        >>> X = DataMat([[1,2,3],[4,5,6]],colnames='cols',idxnames='rows')
        >>> X[0].sum().squeeze()==5
        True
        """
        try:
            return pd.DataFrame.__getitem__(self,key)
        except KeyError: # Perhaps key was for an index?
            return __getitem__(self,(key,))

    def set_index(self,columns,levels=None,inplace=False):
        """Set the DataMat index using existing columns.

        >>> X = DataMat([[1,2,3],[4,5,6]],columns=['a','b','c'],colnames='cols',idxnames='rows')
        >>> X.set_index(['a','b'])
        """
        if inplace:
            frame = self
        else:
            # GH 49473 Use "lazy copy" with Copy-on-Write
            frame = self.copy(deep=None)

        if levels is None:
            levels = columns
            if isinstance(levels,str):
                levels = (levels,)

        try:
            frame.index = pd.MultiIndex.from_frame(pd.DataFrame(frame.reset_index()[columns]))
        except ValueError: # Issue with index vs. multiindex?
            columns = [(i,) for i in columns]
            frame.index = pd.MultiIndex.from_frame(pd.DataFrame(frame.reset_index()[columns]))

        frame.drop(columns,inplace=True,axis=1)
        frame.index.names = levels

        if not inplace:
            return frame

    @property
    def _constructor(self):
        return DataMat

    @property
    def _constructor_sliced(self):
        return DataVec

    def stack(self,**kwargs):
        if 'future_stack' in kwargs.keys():
            return pd.DataFrame.stack(self,**kwargs)
        else:
            return pd.DataFrame.stack(self,future_stack=True,**kwargs)

    # Unary operations
    @cached_property
    def inv(self):
        return DataMat(matrix_inv(self))

    @cached_property
    def norm(self,ord=None,**kwargs):
        return np.linalg.norm(self,ord,**kwargs)

    def dg(self):
        """Extract diagonal from square matrix.

        >>> DataMat([[1,2],[3,4]],idxnames='i').dg().values.tolist()
        [1, 4]
        """
        assert np.all(self.index==self.columns), "Should have columns same as index."
        return DataVec(np.diag(self.values),index=self.index)

    def rank(self,**kwargs):
        """Matrix rank"""
        return np.linalg.matrix_rank(self,**kwargs)

    def svd(self,hermitian=False):
        """Singular value composition into U@S.dg@V.T."""

        u,s,vt = utils.svd(self,hermitian=hermitian)
        u = DataMat(u)
        vt = DataMat(vt)
        s = DataVec(s)

        return u,s,vt

    def eig(self,hermitian=False):
        """Eigendecomposition.  Returns eigenvalues & corresponding eigenvectors.
        """
        s2,u = utils.eig(self,hermitian=hermitian)
        u = DataMat(u)
        s2 = DataVec(s2)

        return s2,u

    def sqrtm(self,hermitian=False):
        return DataMat(utils.sqrtm(self))

    def cholesky(self):
        return DataMat(utils.cholesky(self))

    @cached_property
    def pinv(self):
        """Moore-Penrose pseudo-inverse.

        >>> A = dm.DataMat([[1,2,3],[4,5,6]])
        >>> np.allclose(A@A.pinv,np.eye(2))
        True
        """
        return DataMat(matrix_pinv(self))

    # Binary operations
    def matmul(self,other,strict=False,fillmiss=False):
        Y = matrix_product(self,other,strict=strict,fillmiss=fillmiss)
        if len(Y.shape) <= 1 or Y.shape[1]==1:
            return DataVec(Y)
        else:
            return DataMat(Y)

    __matmul__ = matmul

    def kron(self,other,sparse=False):
        return DataMat(utils.kron(self,other,sparse=sparse))

    def lstsq(self,other):
        rslt = np.linalg.lstsq(self,other,rcond=None)

        if len(rslt[0].shape)<2 or rslt[0].shape[1]==1:
            b = DataVec(rslt[0],index=self.columns)
        else:
            b = DataMat(rslt[0],index=self.columns,columns=other.columns)

        return b

    def proj(self,other):
        b = other.lstsq(self)
        return other@b

    # Other transformations
    def dummies(self,cols,suffix=''):
        return DataMat(utils.dummies(self,cols,suffix=suffix))

    def concat(self,other,axis=0,levelnames=False,toplevelname='v',suffixer='_',
               drop_vestigial_levels=False,**kwargs):
        """Concatenate self and other.

        This uses the machinery of pandas.concat, but ensures that when two
        DataMats having multiindices with different number of levels are
        concatenated that new levels are added so as to preserve a result with a
        multiindex.

        if other is a dictionary and levelnames is not False, then a new level in the multiindex is created naming the columns belonging to the original DataMats.

        USAGE
        -----
        >>> a = DataVec([1,2],name='a',idxnames='i')
        >>> b = DataMat([[1,2],[3,4]],name='b',idxnames='i',colnames='j')
        >>> b.concat([a,b],axis=1,levelnames=True).columns.levels[0].tolist()
        ['b', 'a', 'b_0']
        """
        # Make other a list, unless it's a dict, and get allnames.
        if levelnames==False:
            assign_missing=True
        else:
            assign_missing=levelnames
            levelnames = True

        allobjs = []
        if isinstance(other,dict):
            allobjs = [self] + list(other.values())
            allnames = [self.name] + list(other.keys())
        else:
            if isinstance(other,tuple):
                allobjs = [self] + list(other)
            elif isinstance(other,(DataMat,DataVec)):
                allobjs = [self,other]
                allnames = [self.name] + get_names([other],assign_missing=assign_missing)
            elif isinstance(other,list):
                allobjs = [self]+other
            else:
                raise ValueError("Unexpected type")

            allnames = get_names(allobjs,assign_missing=assign_missing)

        # Have list of all names, but may not be unique.

        suffix = (f'{suffixer}{i:d}' for i in range(len(allnames)))
        unique_names = []
        for i,name in enumerate(allnames):
            if name is None:
                name = next(suffix)
            if name not in unique_names:
                unique_names.append(name)
            else:
                unique_names.append(name+next(suffix))

        # Reconcile indices so they all have common named levels.
        idxs = reconcile_indices([obj.index for obj in allobjs],
                                 drop_vestigial_levels=drop_vestigial_levels)
        for i in range(len(idxs)):
            allobjs[i].index = idxs[i]

        # Get list of columns, allowing for DataVec
        allcols = []
        for i,obj in enumerate(allobjs):
            try:
                allcols += [obj.columns]
            except AttributeError: # No columns attribute?
                obj = DataMat(obj)
                allobjs[i] = obj
                allcols += [obj.columns]
        cols = reconcile_indices(allcols,drop_vestigial_levels=drop_vestigial_levels)
        for i in range(len(idxs)):
            allobjs[i].columns = cols[i]

        # Now have a list of unique names, build a dictionary
        d = dict(zip(unique_names,allobjs))

        if levelnames:
            return utils.concat(d,axis=axis,names=toplevelname,**kwargs)
        else:
            return utils.concat(allobjs,axis=axis,**kwargs)
#+end_src

** datamat utils
#+begin_src python :tangle metrics_miscellany/datamat.py
from lsms.tools import from_dta

def get_names(dms,assign_missing=False):
    """
    Given an iterable of DataMats or DataVecs, return a list of names.

    If an item does not have a name, give "None" unless assign_missing,
    in which case:

       assign_missing==True: use a sequence "_0", "_1", etc.
       assign_missing is a list: Use this list to assign names.

    >>> a = DataVec([1,2],name='a')
    >>> b = DataMat([[1,2]],name='b')
    >>> c = DataMat([[1,2]])

    >>> get_names([a,b,c])
    ['a', 'b', None]

    >>> get_names([a,b,c],assign_missing=True)
    ['a', 'b', '_0']
    """
    names = []
    for item in dms:
        try:
            names += [item.name]
        except AttributeError:
            names += [None]

    if not assign_missing: return names
    else:
        if assign_missing==True:
            missnames = (f'_{i:d}' for i in range(len(names)))
        else:
            missnames = (name for name in assign_missing)

        for i,item in enumerate(names):
            if item is None:
                names[i] = next(missnames)
        return names

def reconcile_indices(idxs,fillvalue='',drop_vestigial_levels=False):
    """
    Given a list of indices, give them all the same levels.

    >>> idx0 = pd.MultiIndex
    """
    # Get union of index level names, preserving order
    names = []
    dropped_level_values = []
    newidxs = []
    for x in idxs:
        # Identify vestigial levels & drop
        droppednames = {}
        for i,level in enumerate(x.levels):
            if drop_vestigial_levels and len(level)==1: # Vestigial level
                try:
                    if len(x.levels)>1:
                        dropname = x.names[i]
                        x = x.droplevel(dropname)
                        droppednames[dropname] = level[0]
                except AttributeError: # May be an index
                    pass
        dropped_level_values.append(droppednames)
        newidxs.append(x)
        for newname in x.names:
            if newname not in names:
                names += [newname]

    # Add levels to indices where necessary
    out = []
    for i,idx in enumerate(newidxs):
        for levelname in names:
            if levelname not in idx.names:
                droppednames = dropped_level_values[i]
                try:
                    fillvalue = droppednames[levelname]
                except KeyError: pass
                idx = utils.concat([DataMat(index=idx)],keys=[fillvalue],names=[levelname]).index
        try: # Duck-typing: Is this an index?
            idx.levels
        except AttributeError:
            idx = pd.MultiIndex([idx],[range(len(idx))],names=idx.names)

        out.append(idx.reorder_levels(names))

    return out

def concat(dms,axis=0,levelnames=False,toplevelname='v',suffixer='_',**kwargs):
    """Concatenate self and other.

    This uses the machinery of pandas.concat, but ensures that when two
    DataMats having multiindices with different number of levels are
    concatenated that new levels are added so as to preserve a result with a
    multiindex.

    if other is a dictionary and levelnames is not False, then a new level in the multiindex is created naming the columns belonging to the original DataMats.

    USAGE
    -----
    >>> a = DataVec([1,2],name='a',idxnames='i')
    >>> b = DataMat([[1,2],[3,4]],name='b',idxnames='i',colnames='j')
    >>> concat([a,b],axis=1,levelnames=True).columns.levels[0].tolist()
    ['b', 'a', 'b_0']
    """

    # Make dms a list, unless it's a dict, and get allnames.
    if levelnames==False:
        assign_missing=True
    else:
        assign_missing=levelnames
        levelnames = True

    allobjs = []
    if isinstance(dms,dict):
        allobjs = list(dms.values())
        allnames = list(dms.keys())
    else:
        if isinstance(dms,tuple):
            allobjs = list(dms)
        elif isinstance(dms,(DataMat,DataVec)):
            allobjs = [dms]
            allnames = get_names([dms],assign_missing=assign_missing)
        elif isinstance(dms,list):
            allobjs = dms
        else:
            raise ValueError("Unexpected type")

        allnames = get_names(allobjs,assign_missing=assign_missing)

    # Have list of all names, but may not be unique.

    suffix = (f'{suffixer}{i:d}' for i in range(len(allnames)))
    unique_names = []
    for i,name in enumerate(allnames):
        if name is None:
            name = next(suffix)
        if name not in unique_names:
            unique_names.append(name)
        else:
            unique_names.append(name+next(suffix))

    # Reconcile indices so they all have common named levels.
    idxs = reconcile_indices([obj.index for obj in allobjs])
    for i in range(len(idxs)):
        allobjs[i].index = idxs[i]

    # Get list of columns, allowing for DataVec
    allcols = []
    for i,obj in enumerate(allobjs):
        try:
            allcols += [obj.columns]
        except AttributeError: # No columns attribute?
            obj = DataMat(obj)
            allobjs[i] = obj
            allcols += [obj.columns]
    cols = reconcile_indices(allcols)
    for i in range(len(idxs)):
        allobjs[i].columns = cols[i]

    # Now have a list of unique names, build a dictionary
    d = dict(zip(unique_names,allobjs))

    if levelnames:
        return utils.concat(d,axis=axis,names=toplevelname,**kwargs)
    else:
        return utils.concat(allobjs,axis=axis,**kwargs)

def read_parquet(fn,**kwargs):
    return DataMat(pd.read_parquet(fn,**kwargs))

def read_pickle(fn,**kwargs):
    return DataMat(pd.read_pickle(fn,**kwargs))

def read_stata(fn,**kwargs):
    return DataMat(from_dta(fn,**kwargs))


if __name__ == "__main__":
    a = DataVec([1,2],name='a',idxnames='i')
    b = DataMat([[1,2]],name='b',idxnames='i',colnames='j')
    c = DataMat([[1,2]],colnames='k')
    d = c.concat([a,b],levelnames=True,axis=1)

    import doctest
    doctest.testmod()


#+end_src
* DataMat/Vec functions
The following utilities work directly with datamat objects.
#+begin_src python :tangle metrics_miscellany/datamat.py
def generalized_eig(A,B):
    """
    Generalized eigenvalue problem for symmetric matrices A & B, B positive definite.

    Roots l solves A@v = l*B@v

    Returns list of roots l and corresponding eigenvectors V.
    """
    from scipy.linalg import eigh

    l,v = eigh(A,B)
    l = l[::-1] # Biggest eigenvalues first
    v = v[:,::-1]

    assert np.all(np.abs((A-l[0]*B)@v[:,0])<1e-10)

    v = DataMat(v,index=A.index)
    l = DataVec(l)

    return l,v


def canonical_variates(X,Y):
    """
    Canonical variates from Canonical Correlation Analysis.

    Returns u,v such that corr^2(Yu[m],Xv[m]) is maximized for m=1,...

    See Hastie-Tibshirani-Friedman (2009) Exercise 3.20 or Rao (1965) 8f.
    """

    m = min(X.shape[1],Y.shape[1])
    U1 = X - X.mean()
    U2 = Y - Y.mean()

    T = U1.shape[0]

    S11 = U1.T@U1/T
    S22 = U2.T@U2/T

    S12 = U1.T@U2/T
    S21 = S12.T

    l,M = generalized_eig(S21@S11.inv@S12,S22)
    l0,L = generalized_eig(S12@S22.inv@S21,S11)

    assert np.allclose(l[:m],l0[:m])

    # Flip signs if necessary to have positive correlations
    sign = np.sign(((S12@M)/(S11@L*np.sqrt(l))).mean())  # cf. Rao 8f.1.2

    # Interpret as a correlation coefficient
    l = np.sqrt(l)

    return l, L*sign, M

def reduced_rank_regression(X,Y,r):
    """
    Reduced rank multivariate regression Y = XB + e.

    Minimizes sum of squared errors subject to requirement that B.rank()==r.

    See Hastie et al (2009) S. 4.2 or She-Chen (2017).
    """

    muX = X.mean()
    muY = Y.mean()

    X = X - muX
    Y = Y - muY

    C = sqrtm(Y.cov())

    U,rho,Vt = ((C@Y.T@(Y.proj(X)))@C).svd()
    V = Vt.T

    Br = X.lstsq(Y@V.iloc[:,:r])@V.iloc[:,:r].pinv

    return Br


#+end_src



* Estimators
** Preliminaries
#+begin_src python :tangle metrics_miscellany/estimators.py
import statsmodels.api as sm
from statsmodels.stats import correlation_tools
import numpy as np
from numpy.linalg import lstsq
import warnings
import pandas as pd
from . import gmm
from . GMM_class import GMM
from . import utils
from .datamat import DataMat, DataVec
#+end_src
** OLS
#+begin_src python :tangle metrics_miscellany/estimators.py

def ols(X,y,cov_type='HC3',PSD_COV=False):
    """OLS estimator of b in y = Xb + u.

    Returns both estimate b as well as an estimate of Var(b).

    The estimator used for the covariance matrix depends on the
    optional argument =cov_type=.

    If optional flag PSD_COV is set, then an effort is made to ensure that
    the estimated covariance matrix is positive semi-definite.  If PSD_COV is
    set to a positive float, then this will be taken to be the smallest eigenvalue
    of the 'corrected' matrix.
    """
    n,k = X.shape

    est = sm.OLS(y,X).fit()
    b = pd.DataFrame({'Coefficients':est.params.values},index=X.columns)
    if cov_type=='HC3':
        V = est.cov_HC3
    elif cov_type=='OLS':
        XX = X.T@X
        if np.linalg.eigh(XX)[0].min()<0:
            XX = correlation_tools.cov_nearest(XX,method='nearest')
            warnings.warn("X'X not positive (semi-) definite.  Correcting!  Estimated variances should not be affected.")
        V = est.resid.var()*np.linalg.inv(XX)
    elif cov_type=='HC2':
        V = est.cov_HC2
    elif cov_type=='HC1':
        V = est.cov_HC1
    elif cov_type=='HC0':
        V = est.cov_HC0
    else:
        raise ValueError("Unknown type of covariance matrix.")

    if PSD_COV:
        if PSD_COV is True:
            PSD_COV = (b**2).min()
        s,U = np.linalg.eigh((V+V.T)/2)
        if s.min()<PSD_COV:
            oldV = V
            V = U@np.diag(np.maximum(s,PSD_COV))@U.T
            warnings.warn("Estimated covariance matrix not positive (semi-) definite.\nCorrecting! Norm of difference is %g." % np.linalg.norm(oldV-V))

    V = pd.DataFrame(V,index=X.columns,columns=X.columns)

    return b,V

#+end_src

*** OLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_ols.py
import pandas as pd
from metrics_miscellany.estimators import ols
import numpy as np

def test_ols(N=500000,tol=1e-2):

    x = pd.DataFrame({'x':np.random.standard_normal((N,))})
    x['Constant'] = 1

    beta = pd.DataFrame({'Coefficients':[1,0]},index=['x','Constant'])

    u = pd.DataFrame(np.random.standard_normal((N,))/10)

    y = (x@beta).values + u.values
    b,V = ols(x,y)

    assert np.allclose(b,beta,atol=tol)

if __name__=='__main__':
    test_ols()

#+end_src
** Two-stage Least Squares
#+begin_src python :tangle metrics_miscellany/estimators.py
def tsls(X,y,Z,return_Omega=False):
    """
    Two-stage least squares estimator.
    """

    n,k = X.shape

    Qxz = X.T@Z/n

    zzinv = utils.inv(Z.T@Z/n)
    b = lstsq(Qxz@zzinv@Qxz.T,Qxz@zzinv@Z.T@y/n,rcond=None)[0]

    b = pd.Series(b.squeeze(),index=X.columns)

    # Cov matrix
    e = y.squeeze() - X@b

    #Omega = Z.T@(e**2).dg()@Z/n
    # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
    ZTe = Z.T.multiply(e)
    Omega = ZTe@ZTe.T/n

    #Omega = pd.DataFrame(e.var()*Z.T.values@Z.values/n,columns=Z.columns,index=Z.columns)

    if return_Omega:
        return b,Omega
    else:
        A = (Qxz@zzinv@Qxz.T).inv
        V = A@(Qxz@zzinv@Omega@zzinv@Qxz.T)@A.T/n
        return b,V

#+end_src
*** TSLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_tsls.py
import pandas as pd
from metrics_miscellany.estimators import tsls, ols
from metrics_miscellany.datamat import DataMat, DataVec
import numpy as np

def test_tsls(N=500000,tol=1e-2):

    z = DataMat({'z':np.random.standard_normal((N,))})
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.squeeze() + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    #b,V = tsls(x,y,z)
    b,V = ols(x,y)

    assert np.allclose(b,beta.squeeze(),atol=tol)

if __name__=='__main__':
    test_tsls()

#+end_src
